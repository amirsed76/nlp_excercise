{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk \n",
    "import numpy\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 1\n",
    "read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sport.txt\",\"r\") as file:\n",
    "    sport_text = file.read()\n",
    "    sport_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tech.txt\",\"r\") as file :\n",
    "    tech_text = file.read()\n",
    "    tech_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 2 & 3\n",
    "synsets of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "==========================================\n",
      "pharmacy\n",
      "\n",
      "_________________________________\n",
      "[synset 1]\n",
      "Synset name :   pharmacy.n.01\n",
      "Synset meaning :  the art and science of preparing and dispensing drugs and medicines,\n",
      "Synset example :  []\n",
      "Synset abstract term :   [Synset('medicine.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 2]\n",
      "Synset name :   drugstore.n.01\n",
      "Synset meaning :  a retail shop where medicine and other articles are sold\n",
      "Synset example :  []\n",
      "Synset abstract term :   [Synset('shop.n.01')]\n",
      "Synset tag :  n\n",
      "==========================================\n",
      "==========================================\n",
      "flower\n",
      "\n",
      "_________________________________\n",
      "[synset 1]\n",
      "Synset name :   flower.n.01\n",
      "Synset meaning :  a plant cultivated for its blooms or blossoms\n",
      "Synset example :  []\n",
      "Synset abstract term :   [Synset('angiosperm.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 2]\n",
      "Synset name :   flower.n.02\n",
      "Synset meaning :  reproductive organ of angiosperm plants especially one having showy or colorful parts\n",
      "Synset example :  []\n",
      "Synset abstract term :   [Synset('reproductive_structure.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 3]\n",
      "Synset name :   flower.n.03\n",
      "Synset meaning :  the period of greatest prosperity or productivity\n",
      "Synset example :  []\n",
      "Synset abstract term :   [Synset('time_period.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 4]\n",
      "Synset name :   bloom.v.01\n",
      "Synset meaning :  produce or yield flowers\n",
      "Synset example :  ['The cherry tree bloomed']\n",
      "Synset abstract term :   [Synset('develop.v.10')]\n",
      "Synset tag :  v\n",
      "==========================================\n",
      "==========================================\n",
      "dog\n",
      "\n",
      "_________________________________\n",
      "[synset 1]\n",
      "Synset name :   dog.n.01\n",
      "Synset meaning :  a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "Synset example :  ['the dog barked all night']\n",
      "Synset abstract term :   [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 2]\n",
      "Synset name :   frump.n.01\n",
      "Synset meaning :  a dull unattractive unpleasant girl or woman\n",
      "Synset example :  ['she got a reputation as a frump', \"she's a real dog\"]\n",
      "Synset abstract term :   [Synset('unpleasant_woman.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 3]\n",
      "Synset name :   dog.n.03\n",
      "Synset meaning :  informal term for a man\n",
      "Synset example :  ['you lucky dog']\n",
      "Synset abstract term :   [Synset('chap.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 4]\n",
      "Synset name :   cad.n.01\n",
      "Synset meaning :  someone who is morally reprehensible\n",
      "Synset example :  ['you dirty dog']\n",
      "Synset abstract term :   [Synset('villain.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 5]\n",
      "Synset name :   frank.n.02\n",
      "Synset meaning :  a smooth-textured sausage of minced beef or pork usually smoked; often served on a bread roll\n",
      "Synset example :  []\n",
      "Synset abstract term :   [Synset('sausage.n.01')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 6]\n",
      "Synset name :   pawl.n.01\n",
      "Synset meaning :  a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
      "Synset example :  []\n",
      "Synset abstract term :   [Synset('catch.n.06')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 7]\n",
      "Synset name :   andiron.n.01\n",
      "Synset meaning :  metal supports for logs in a fireplace\n",
      "Synset example :  ['the andirons were too hot to touch']\n",
      "Synset abstract term :   [Synset('support.n.10')]\n",
      "Synset tag :  n\n",
      "\n",
      "_________________________________\n",
      "[synset 8]\n",
      "Synset name :   chase.v.01\n",
      "Synset meaning :  go after with the intent to catch\n",
      "Synset example :  ['The policeman chased the mugger down the alley', 'the dog chased the rabbit']\n",
      "Synset abstract term :   [Synset('pursue.v.02')]\n",
      "Synset tag :  v\n"
     ]
    }
   ],
   "source": [
    "for word in [\"pharmacy\",\"flower\",\"dog\"] :\n",
    "    print(f\"==========================================\\n==========================================\\n{word}\")\n",
    "    for index , syn in enumerate(wordnet.synsets(word)):\n",
    "        print(f\"\\n_________________________________\\n[synset {index+1}]\")\n",
    "        print (\"Synset name :  \", syn.name())\n",
    "        print (\"Synset meaning : \", syn.definition())\n",
    "        print (\"Synset example : \", syn.examples())\n",
    "        print (\"Synset abstract term :  \", syn.hypernyms())\n",
    "        print (\"Synset tag : \", syn.pos())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(word , print_lemmas = False):\n",
    "    if print_lemmas :\n",
    "        for index , syn in enumerate(wordnet.synsets(word)):\n",
    "            print(f\"\\n_________________________________\\n[synset {index+1}] {syn.name()}\")\n",
    "            for i , lemma in enumerate(syn.lemmas()):\n",
    "                print(f\"[{i}] : \" , lemma)\n",
    "\n",
    "    return set([lemma for synset in wordnet.synsets(word) for lemma in synset.lemmas()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________________\n",
      "[synset 1] banks.n.01\n",
      "[0] :  Lemma('banks.n.01.Banks')\n",
      "[1] :  Lemma('banks.n.01.Sir_Joseph_Banks')\n",
      "\n",
      "_________________________________\n",
      "[synset 2] bank.n.01\n",
      "[0] :  Lemma('bank.n.01.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 3] depository_financial_institution.n.01\n",
      "[0] :  Lemma('depository_financial_institution.n.01.depository_financial_institution')\n",
      "[1] :  Lemma('depository_financial_institution.n.01.bank')\n",
      "[2] :  Lemma('depository_financial_institution.n.01.banking_concern')\n",
      "[3] :  Lemma('depository_financial_institution.n.01.banking_company')\n",
      "\n",
      "_________________________________\n",
      "[synset 4] bank.n.03\n",
      "[0] :  Lemma('bank.n.03.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 5] bank.n.04\n",
      "[0] :  Lemma('bank.n.04.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 6] bank.n.05\n",
      "[0] :  Lemma('bank.n.05.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 7] bank.n.06\n",
      "[0] :  Lemma('bank.n.06.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 8] bank.n.07\n",
      "[0] :  Lemma('bank.n.07.bank')\n",
      "[1] :  Lemma('bank.n.07.cant')\n",
      "[2] :  Lemma('bank.n.07.camber')\n",
      "\n",
      "_________________________________\n",
      "[synset 9] savings_bank.n.02\n",
      "[0] :  Lemma('savings_bank.n.02.savings_bank')\n",
      "[1] :  Lemma('savings_bank.n.02.coin_bank')\n",
      "[2] :  Lemma('savings_bank.n.02.money_box')\n",
      "[3] :  Lemma('savings_bank.n.02.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 10] bank.n.09\n",
      "[0] :  Lemma('bank.n.09.bank')\n",
      "[1] :  Lemma('bank.n.09.bank_building')\n",
      "\n",
      "_________________________________\n",
      "[synset 11] bank.n.10\n",
      "[0] :  Lemma('bank.n.10.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 12] bank.v.01\n",
      "[0] :  Lemma('bank.v.01.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 13] bank.v.02\n",
      "[0] :  Lemma('bank.v.02.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 14] bank.v.03\n",
      "[0] :  Lemma('bank.v.03.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 15] bank.v.04\n",
      "[0] :  Lemma('bank.v.04.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 16] bank.v.05\n",
      "[0] :  Lemma('bank.v.05.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 17] deposit.v.02\n",
      "[0] :  Lemma('deposit.v.02.deposit')\n",
      "[1] :  Lemma('deposit.v.02.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 18] bank.v.07\n",
      "[0] :  Lemma('bank.v.07.bank')\n",
      "\n",
      "_________________________________\n",
      "[synset 19] trust.v.01\n",
      "[0] :  Lemma('trust.v.01.trust')\n",
      "[1] :  Lemma('trust.v.01.swear')\n",
      "[2] :  Lemma('trust.v.01.rely')\n",
      "[3] :  Lemma('trust.v.01.bank')\n"
     ]
    }
   ],
   "source": [
    "banks_lemma = get_lemmas(\"banks\",print_lemmas = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_________________________________\n",
      "[synset 1] sung.n.01\n",
      "[0] :  Lemma('sung.n.01.Sung')\n",
      "[1] :  Lemma('sung.n.01.Sung_dynasty')\n",
      "[2] :  Lemma('sung.n.01.Song')\n",
      "[3] :  Lemma('sung.n.01.Song_dynasty')\n",
      "\n",
      "_________________________________\n",
      "[synset 2] sing.v.01\n",
      "[0] :  Lemma('sing.v.01.sing')\n",
      "\n",
      "_________________________________\n",
      "[synset 3] sing.v.02\n",
      "[0] :  Lemma('sing.v.02.sing')\n",
      "\n",
      "_________________________________\n",
      "[synset 4] sing.v.03\n",
      "[0] :  Lemma('sing.v.03.sing')\n",
      "\n",
      "_________________________________\n",
      "[synset 5] whistle.v.05\n",
      "[0] :  Lemma('whistle.v.05.whistle')\n",
      "[1] :  Lemma('whistle.v.05.sing')\n",
      "\n",
      "_________________________________\n",
      "[synset 6] spill_the_beans.v.01\n",
      "[0] :  Lemma('spill_the_beans.v.01.spill_the_beans')\n",
      "[1] :  Lemma('spill_the_beans.v.01.let_the_cat_out_of_the_bag')\n",
      "[2] :  Lemma('spill_the_beans.v.01.talk')\n",
      "[3] :  Lemma('spill_the_beans.v.01.tattle')\n",
      "[4] :  Lemma('spill_the_beans.v.01.blab')\n",
      "[5] :  Lemma('spill_the_beans.v.01.peach')\n",
      "[6] :  Lemma('spill_the_beans.v.01.babble')\n",
      "[7] :  Lemma('spill_the_beans.v.01.sing')\n",
      "[8] :  Lemma('spill_the_beans.v.01.babble_out')\n",
      "[9] :  Lemma('spill_the_beans.v.01.blab_out')\n"
     ]
    }
   ],
   "source": [
    "sung_lemma = get_lemmas(\"sung\" , print_lemmas = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_opposite(word):\n",
    "    lemmas = get_lemmas(word)\n",
    "    print(f\"____________________\\nsynonyms of {word} \\n \")\n",
    "    for lemma in lemmas :\n",
    "        print(lemma.name())\n",
    "        \n",
    "    print(f\"__________________________ \\nopposite of {word}\\n\")\n",
    "    for lemma in lemmas:\n",
    "        for antonym in lemma.antonyms():\n",
    "            print(antonym.name())\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      " good\n",
      "===========================================\n",
      "____________________\n",
      "synonyms of good \n",
      " \n",
      "unspoiled\n",
      "skilful\n",
      "dependable\n",
      "salutary\n",
      "beneficial\n",
      "undecomposed\n",
      "proficient\n",
      "expert\n",
      "good\n",
      "soundly\n",
      "safe\n",
      "dear\n",
      "just\n",
      "in_force\n",
      "upright\n",
      "effective\n",
      "right\n",
      "serious\n",
      "secure\n",
      "thoroughly\n",
      "adept\n",
      "trade_good\n",
      "skillful\n",
      "near\n",
      "goodness\n",
      "full\n",
      "sound\n",
      "practiced\n",
      "honorable\n",
      "commodity\n",
      "well\n",
      "honest\n",
      "unspoilt\n",
      "estimable\n",
      "respectable\n",
      "in_effect\n",
      "ripe\n",
      "__________________________ \n",
      "opposite of good\n",
      "\n",
      "evilness\n",
      "===========================================\n",
      " better\n",
      "===========================================\n",
      "____________________\n",
      "synonyms of better \n",
      " \n",
      "unspoiled\n",
      "considerably\n",
      "skilful\n",
      "dependable\n",
      "comfortably\n",
      "salutary\n",
      "beneficial\n",
      "wagerer\n",
      "undecomposed\n",
      "proficient\n",
      "expert\n",
      "punter\n",
      "good\n",
      "better\n",
      "improve\n",
      "ameliorate\n",
      "safe\n",
      "dear\n",
      "just\n",
      "in_force\n",
      "upright\n",
      "amend\n",
      "effective\n",
      "right\n",
      "break\n",
      "serious\n",
      "easily\n",
      "secure\n",
      "adept\n",
      "best\n",
      "bettor\n",
      "skillful\n",
      "near\n",
      "full\n",
      "sound\n",
      "practiced\n",
      "honorable\n",
      "well\n",
      "honest\n",
      "substantially\n",
      "unspoilt\n",
      "advantageously\n",
      "estimable\n",
      "respectable\n",
      "in_effect\n",
      "intimately\n",
      "ripe\n",
      "meliorate\n",
      "__________________________ \n",
      "opposite of better\n",
      "\n",
      "bad\n",
      "disadvantageously\n",
      "===========================================\n",
      " dark\n",
      "===========================================\n",
      "____________________\n",
      "synonyms of dark \n",
      " \n",
      "drab\n",
      "night\n",
      "dismal\n",
      "sullen\n",
      "dark-skinned\n",
      "gloomy\n",
      "glowering\n",
      "disconsolate\n",
      "sinister\n",
      "sour\n",
      "blue\n",
      "drear\n",
      "dreary\n",
      "saturnine\n",
      "wickedness\n",
      "nighttime\n",
      "coloured\n",
      "moody\n",
      "iniquity\n",
      "grim\n",
      "dingy\n",
      "non-white\n",
      "black\n",
      "sorry\n",
      "morose\n",
      "shadow\n",
      "benighted\n",
      "dour\n",
      "colored\n",
      "glum\n",
      "dark\n",
      "darkness\n",
      "obscure\n",
      "__________________________ \n",
      "opposite of dark\n",
      "\n",
      "day\n",
      "light\n",
      "===========================================\n",
      " long\n",
      "===========================================\n",
      "____________________\n",
      "synonyms of long \n",
      " \n",
      "recollective\n",
      "hanker\n",
      "prospicient\n",
      "foresightful\n",
      "foresighted\n",
      "farsighted\n",
      "retentive\n",
      "long\n",
      "longsighted\n",
      "yearn\n",
      "farseeing\n",
      "tenacious\n",
      "__________________________ \n",
      "opposite of long\n",
      "\n",
      "unretentive\n",
      "===========================================\n",
      " car\n",
      "===========================================\n",
      "____________________\n",
      "synonyms of car \n",
      " \n",
      "motorcar\n",
      "car\n",
      "gondola\n",
      "elevator_car\n",
      "automobile\n",
      "machine\n",
      "railway_car\n",
      "railroad_car\n",
      "railcar\n",
      "cable_car\n",
      "auto\n",
      "__________________________ \n",
      "opposite of car\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in [\"good\" , \"better\" , \"dark\" , \"long\" ,\"car\"]:\n",
    "    print(f\"===========================================\\n {word}\\n===========================================\")\n",
    "    synonym_opposite(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms_and_Hyponyms(word):\n",
    "    hypernyms = []\n",
    "    hyponyms=[]\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for hypernym in syn.hypernyms():\n",
    "            hypernyms.append(hypernym)\n",
    "        for hyponym in syn.hyponyms():\n",
    "            hyponyms.append(hyponym) \n",
    "            \n",
    "    return set(hypernyms),set(hyponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "=======================\n",
      "word = car\n",
      "\n",
      "hypernyms:\n",
      "\n",
      "motor_vehicle.n.01\n",
      "wheeled_vehicle.n.01\n",
      "compartment.n.02\n",
      "______________________\n",
      "hyponyms:\n",
      "\n",
      "stock_car.n.01\n",
      "van.n.03\n",
      "loaner.n.02\n",
      "minicar.n.01\n",
      "hardtop.n.01\n",
      "baggage_car.n.01\n",
      "cab.n.03\n",
      "horseless_carriage.n.01\n",
      "freight_car.n.01\n",
      "convertible.n.01\n",
      "hot_rod.n.01\n",
      "guard's_van.n.01\n",
      "compact.n.03\n",
      "limousine.n.01\n",
      "pace_car.n.01\n",
      "subcompact.n.01\n",
      "handcar.n.01\n",
      "cruiser.n.01\n",
      "gas_guzzler.n.01\n",
      "jeep.n.01\n",
      "racer.n.02\n",
      "stanley_steamer.n.01\n",
      "beach_wagon.n.01\n",
      "sedan.n.01\n",
      "used-car.n.01\n",
      "electric.n.01\n",
      "minivan.n.01\n",
      "model_t.n.01\n",
      "bus.n.04\n",
      "roadster.n.01\n",
      "sport_utility.n.01\n",
      "ambulance.n.01\n",
      "club_car.n.01\n",
      "passenger_car.n.01\n",
      "tender.n.04\n",
      "touring_car.n.01\n",
      "slip_coach.n.01\n",
      "hatchback.n.01\n",
      "sports_car.n.01\n",
      "cabin_car.n.01\n",
      "coupe.n.01\n",
      "mail_car.n.01\n",
      "=======================\n",
      "=======================\n",
      "word = frog\n",
      "\n",
      "hypernyms:\n",
      "\n",
      "frenchman.n.01\n",
      "adornment.n.01\n",
      "amphibian.n.03\n",
      "capture.v.06\n",
      "______________________\n",
      "hyponyms:\n",
      "\n",
      "true_toad.n.01\n",
      "liopelma_hamiltoni.n.01\n",
      "tree_toad.n.01\n",
      "tongueless_frog.n.01\n",
      "barking_frog.n.01\n",
      "midwife_toad.n.01\n",
      "sheep_frog.n.01\n",
      "true_frog.n.01\n",
      "obstetrical_toad.n.01\n",
      "fire-bellied_toad.n.01\n",
      "south_american_poison_toad.n.01\n",
      "eastern_narrow-mouthed_toad.n.01\n",
      "spadefoot.n.01\n",
      "western_narrow-mouthed_toad.n.01\n",
      "tailed_frog.n.01\n",
      "crapaud.n.01\n",
      "leptodactylid_frog.n.01\n",
      "tree_frog.n.02\n",
      "robber_frog.n.02\n",
      "=======================\n",
      "=======================\n",
      "word = tree\n",
      "\n",
      "hypernyms:\n",
      "\n",
      "elongate.v.01\n",
      "steer.v.01\n",
      "chase.v.01\n",
      "woody_plant.n.01\n",
      "plane_figure.n.01\n",
      "plant.v.01\n",
      "______________________\n",
      "hyponyms:\n",
      "\n",
      "silver_ash.n.01\n",
      "gliricidia.n.01\n",
      "burma_padauk.n.01\n",
      "coralwood.n.01\n",
      "cladogram.n.01\n",
      "quandong.n.01\n",
      "cocobolo.n.01\n",
      "bayberry.n.01\n",
      "brazilian_pepper_tree.n.01\n",
      "clusia.n.01\n",
      "idesia.n.01\n",
      "stemma.n.01\n",
      "bloodwood_tree.n.01\n",
      "necklace_tree.n.01\n",
      "christmas_bush.n.01\n",
      "kino.n.02\n",
      "gymnospermous_tree.n.01\n",
      "devilwood.n.01\n",
      "bonduc.n.02\n",
      "maria.n.02\n",
      "african_walnut.n.01\n",
      "red_silk-cotton_tree.n.01\n",
      "quira.n.02\n",
      "tree_of_knowledge.n.01\n",
      "shingle_tree.n.01\n",
      "elm.n.01\n",
      "chaulmoogra.n.01\n",
      "southern_beech.n.01\n",
      "white_mangrove.n.01\n",
      "linden.n.02\n",
      "bean_tree.n.01\n",
      "angelim.n.01\n",
      "willow.n.01\n",
      "dagame.n.01\n",
      "calabash.n.02\n",
      "giant_chinkapin.n.01\n",
      "cockspur.n.02\n",
      "pandanus.n.02\n",
      "caracolito.n.01\n",
      "spanish_tamarind.n.01\n",
      "silver_tree.n.02\n",
      "palm.n.03\n",
      "granadilla_tree.n.01\n",
      "divi-divi.n.02\n",
      "guama.n.01\n",
      "guinea_pepper.n.02\n",
      "hackberry.n.01\n",
      "jamaican_cherry.n.01\n",
      "lancewood.n.02\n",
      "alder.n.02\n",
      "dipterocarp.n.01\n",
      "hop_hornbeam.n.01\n",
      "marmalade_tree.n.01\n",
      "gutta-percha_tree.n.02\n",
      "neem.n.01\n",
      "lead_tree.n.01\n",
      "jamaica_dogwood.n.01\n",
      "obeche.n.02\n",
      "plane_tree.n.01\n",
      "black_mangrove.n.01\n",
      "rose_chestnut.n.01\n",
      "wild_medlar.n.01\n",
      "birch.n.02\n",
      "dhawa.n.01\n",
      "kentucky_coffee_tree.n.01\n",
      "tulipwood_tree.n.01\n",
      "pepper_tree.n.02\n",
      "yellowwood.n.02\n",
      "red_sandalwood.n.02\n",
      "wild_fig.n.02\n",
      "chestnut.n.02\n",
      "acacia.n.01\n",
      "mayeng.n.01\n",
      "kingwood.n.02\n",
      "shade_tree.n.01\n",
      "button_tree.n.01\n",
      "lanseh_tree.n.01\n",
      "lemonwood.n.02\n",
      "manila_tamarind.n.01\n",
      "lacebark.n.01\n",
      "dhak.n.01\n",
      "kowhai.n.01\n",
      "mescal_bean.n.01\n",
      "zebrawood.n.02\n",
      "montezuma.n.01\n",
      "breakax.n.01\n",
      "japanese_pagoda_tree.n.01\n",
      "incense_tree.n.01\n",
      "lepidobotrys.n.01\n",
      "laurelwood.n.01\n",
      "snag.n.02\n",
      "palo_verde.n.01\n",
      "oak.n.02\n",
      "bonsai.n.01\n",
      "fever_tree.n.01\n",
      "bitterwood_tree.n.01\n",
      "gum_tree.n.01\n",
      "camwood.n.01\n",
      "winter's_bark.n.02\n",
      "ebony.n.03\n",
      "prickly_ash.n.02\n",
      "locust_tree.n.01\n",
      "tanbark_oak.n.01\n",
      "cabbage_tree.n.03\n",
      "padauk.n.01\n",
      "conacaste.n.01\n",
      "teak.n.02\n",
      "inga.n.01\n",
      "white_mangrove.n.02\n",
      "wild_tamarind.n.02\n",
      "bottle-tree.n.01\n",
      "ribbon_tree.n.01\n",
      "trifoliate_orange.n.01\n",
      "fig_tree.n.01\n",
      "ketembilla.n.01\n",
      "calaba.n.01\n",
      "beech.n.01\n",
      "hydnocarpus_laurifolia.n.01\n",
      "keurboom.n.01\n",
      "timber_tree.n.01\n",
      "msasa.n.01\n",
      "peruvian_balsam.n.01\n",
      "sandalwood_tree.n.01\n",
      "hazel.n.01\n",
      "mahogany.n.02\n",
      "gutta-percha_tree.n.01\n",
      "tolu_tree.n.01\n",
      "nitta_tree.n.01\n",
      "ice-cream_bean.n.01\n",
      "rosewood.n.02\n",
      "nakedwood.n.01\n",
      "coral_tree.n.01\n",
      "australian_nettle.n.01\n",
      "cassia.n.01\n",
      "wheel_tree.n.01\n",
      "angiospermous_tree.n.01\n",
      "satinwood.n.03\n",
      "hornbeam.n.01\n",
      "cork_tree.n.01\n",
      "anise_tree.n.01\n",
      "marblewood.n.02\n",
      "shaving-brush_tree.n.01\n",
      "indian_beech.n.01\n",
      "prickly_ash.n.01\n",
      "chinese_parasol_tree.n.01\n",
      "cinchona.n.02\n",
      "oak_chestnut.n.01\n",
      "brazilwood.n.02\n",
      "casuarina.n.01\n",
      "sapling.n.01\n",
      "tipu.n.01\n",
      "keurboom.n.02\n",
      "turreae.n.01\n",
      "ash.n.02\n",
      "silver_tree.n.01\n",
      "dita.n.01\n",
      "balata.n.02\n",
      "aroeira_blanca.n.01\n",
      "albizzia.n.01\n",
      "brazilian_ironwood.n.01\n",
      "carib_wood.n.01\n",
      "pepper_tree.n.01\n",
      "treelet.n.01\n",
      "soapberry.n.01\n",
      "pollard.n.01\n",
      "quandong.n.03\n",
      "millettia.n.01\n",
      "princewood.n.01\n",
      "opepe.n.01\n",
      "poon.n.02\n",
      "aalii.n.01\n",
      "sissoo.n.01\n",
      "chinaberry.n.02\n",
      "coffee.n.02\n",
      "fringe_tree.n.01\n",
      "puka.n.02\n",
      "scrub_beefwood.n.01\n",
      "souari.n.01\n",
      "ivory_tree.n.01\n",
      "arbor.n.01\n",
      "blackwood.n.02\n",
      "scarlet_wisteria_tree.n.01\n"
     ]
    }
   ],
   "source": [
    "for word in [\"car\",\"frog\",\"tree\"]:\n",
    "    print(f\"=======================\\n=======================\\nword = {word}\\n\\nhypernyms:\\n\")\n",
    "    hypernyms,hyponyms = get_hypernyms_and_Hyponyms(word)\n",
    "    for hypernym in hypernyms:\n",
    "        print(hypernym.name())\n",
    "    print(\"______________________\\nhyponyms:\\n\")\n",
    "    for hyponym in hyponyms:\n",
    "        print(hyponym.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 7 \n",
    "## part 7-a\n",
    "root hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word = Dog     => ['entity.n.01']\n",
      "word = Cat     => ['entity.n.01']\n",
      "word = Car     => ['entity.n.01']\n",
      "word = Bicycle => ['entity.n.01']\n",
      "word = Tree    => ['entity.n.01']\n",
      "word = Flower  => ['entity.n.01']\n",
      "word = Water   => ['entity.n.01']\n",
      "word = Rainbow => ['entity.n.01']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"Dog\",\"Cat\",\"Car\",\"Bicycle\",\"Tree\",\"Flower\",\"Water\",\"Rainbow\"]:\n",
    "    root_hypernyms = wordnet.synsets(word)[0].root_hypernyms()\n",
    "    print(f\"word = {word}\".ljust(15) +f\"=> {[item.name() for item in root_hypernyms]}\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 7-b\n",
    "Lowest common hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for Dog & Cat       => ['carnivore.n.01']\n",
      "for Car & Bicycle   => ['wheeled_vehicle.n.01']\n",
      "for Tree & Flower   => ['vascular_plant.n.01']\n",
      "for Water & Rainbow => ['abstraction.n.06']\n"
     ]
    }
   ],
   "source": [
    "for word1,word2 in [(\"Dog\",\"Cat\"),(\"Car\",\"Bicycle\"),(\"Tree\" ,\"Flower\"),(\"Water\",\"Rainbow\")]:\n",
    "    syn1 = wordnet.synsets(word1)[0]\n",
    "    syn2 = wordnet.synsets(word2)[0]\n",
    "    lowest_common_hypernyms = syn1.lowest_common_hypernyms(syn2)\n",
    "    print(f\"for {word1} & {word2}\".ljust(20) +f\"=> {[item.name() for item in lowest_common_hypernyms]}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Dog\",\"Cat\",\"Car\",\"Bicycle\",\"Tree\",\"Flower\",\"Water\",\"Rainbow\"]\n",
    "path_similarities = numpy.zeros((8,8))\n",
    "resnik_similarities = numpy.zeros((8,8))\n",
    "jiang_conrath_similarities = numpy.zeros((8,8))\n",
    "\n",
    "for index1 , word1 in enumerate(words):\n",
    "    for index2 , word2 in enumerate(words):\n",
    "        syn1 = wordnet.synsets(word1)[0]\n",
    "        syn2 = wordnet.synsets(word2)[0]\n",
    "        \n",
    "        # path similarity\n",
    "        path_similarities[index1,index2]=syn1.path_similarity(syn2)\n",
    "        \n",
    "        # resnik_similarity \n",
    "        resnik_similarities[index1,index2] = syn1.res_similarity(syn2,wordnet_ic.ic(\"ic-brown.dat\"))\n",
    "        \n",
    "        #jiang_conrath_similarity\n",
    "        jiang_conrath_similarities[index1,index2]=syn1.jcn_similarity(syn2,wordnet_ic.ic(\"ic-brown.dat\"))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similary_word(row , columns ):\n",
    "    return sorted(columns , key=lambda item:row[item])[-2]    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### path_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "      <th>Car</th>\n",
       "      <th>Bicycle</th>\n",
       "      <th>Tree</th>\n",
       "      <th>Flower</th>\n",
       "      <th>Water</th>\n",
       "      <th>Rainbow</th>\n",
       "      <th>most_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dog</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bicycle</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tree</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>Flower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flower</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>Tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rainbow</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Water</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Dog       Cat       Car   Bicycle      Tree    Flower     Water  \\\n",
       "Dog      1.000000  0.200000  0.076923  0.090909  0.125000  0.111111  0.083333   \n",
       "Cat      0.200000  1.000000  0.055556  0.062500  0.076923  0.071429  0.058824   \n",
       "Car      0.076923  0.055556  1.000000  0.200000  0.071429  0.066667  0.071429   \n",
       "Bicycle  0.090909  0.062500  0.200000  1.000000  0.083333  0.076923  0.083333   \n",
       "Tree     0.125000  0.076923  0.071429  0.083333  1.000000  0.166667  0.076923   \n",
       "Flower   0.111111  0.071429  0.066667  0.076923  0.166667  1.000000  0.071429   \n",
       "Water    0.083333  0.058824  0.071429  0.083333  0.076923  0.071429  1.000000   \n",
       "Rainbow  0.062500  0.047619  0.055556  0.062500  0.058824  0.055556  0.076923   \n",
       "\n",
       "          Rainbow most_similarity  \n",
       "Dog      0.062500             Cat  \n",
       "Cat      0.047619             Dog  \n",
       "Car      0.055556         Bicycle  \n",
       "Bicycle  0.062500             Car  \n",
       "Tree     0.058824          Flower  \n",
       "Flower   0.055556            Tree  \n",
       "Water    0.076923         Bicycle  \n",
       "Rainbow  1.000000           Water  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(path_similarities , columns=words , index=words)\n",
    "df[\"most_similarity\"] = df.apply(lambda row:most_similary_word(row , df.columns) , axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnik_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "      <th>Car</th>\n",
       "      <th>Bicycle</th>\n",
       "      <th>Tree</th>\n",
       "      <th>Flower</th>\n",
       "      <th>Water</th>\n",
       "      <th>Rainbow</th>\n",
       "      <th>most_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dog</th>\n",
       "      <td>9.006014</td>\n",
       "      <td>7.911667</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>2.224150</td>\n",
       "      <td>2.224150</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat</th>\n",
       "      <td>7.911667</td>\n",
       "      <td>9.040650</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>2.224150</td>\n",
       "      <td>2.224150</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car</th>\n",
       "      <td>1.531834</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>7.591401</td>\n",
       "      <td>6.452257</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bicycle</th>\n",
       "      <td>1.531834</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>6.452257</td>\n",
       "      <td>9.250664</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tree</th>\n",
       "      <td>2.224150</td>\n",
       "      <td>2.224150</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>7.764869</td>\n",
       "      <td>6.028316</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>Flower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flower</th>\n",
       "      <td>2.224150</td>\n",
       "      <td>2.224150</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>1.531834</td>\n",
       "      <td>6.028316</td>\n",
       "      <td>8.295989</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>Tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water</th>\n",
       "      <td>0.801759</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>0.801759</td>\n",
       "      <td>8.206018</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>Flower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rainbow</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.596229</td>\n",
       "      <td>12.856162</td>\n",
       "      <td>Water</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Dog       Cat       Car   Bicycle      Tree    Flower     Water  \\\n",
       "Dog      9.006014  7.911667  1.531834  1.531834  2.224150  2.224150  0.801759   \n",
       "Cat      7.911667  9.040650  1.531834  1.531834  2.224150  2.224150  0.801759   \n",
       "Car      1.531834  1.531834  7.591401  6.452257  1.531834  1.531834  0.801759   \n",
       "Bicycle  1.531834  1.531834  6.452257  9.250664  1.531834  1.531834  0.801759   \n",
       "Tree     2.224150  2.224150  1.531834  1.531834  7.764869  6.028316  0.801759   \n",
       "Flower   2.224150  2.224150  1.531834  1.531834  6.028316  8.295989  0.801759   \n",
       "Water    0.801759  0.801759  0.801759  0.801759  0.801759  0.801759  8.206018   \n",
       "Rainbow -0.000000 -0.000000 -0.000000 -0.000000 -0.000000 -0.000000  0.596229   \n",
       "\n",
       "           Rainbow most_similarity  \n",
       "Dog      -0.000000             Cat  \n",
       "Cat      -0.000000             Dog  \n",
       "Car      -0.000000         Bicycle  \n",
       "Bicycle  -0.000000             Car  \n",
       "Tree     -0.000000          Flower  \n",
       "Flower   -0.000000            Tree  \n",
       "Water     0.596229          Flower  \n",
       "Rainbow  12.856162           Water  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(resnik_similarities , columns=words , index=words)\n",
    "df[\"most_similarity\"] = df.apply(lambda row:most_similary_word(row , df.columns) , axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jiang_conrath_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dog</th>\n",
       "      <th>Cat</th>\n",
       "      <th>Car</th>\n",
       "      <th>Bicycle</th>\n",
       "      <th>Tree</th>\n",
       "      <th>Flower</th>\n",
       "      <th>Water</th>\n",
       "      <th>Rainbow</th>\n",
       "      <th>most_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dog</th>\n",
       "      <td>inf</td>\n",
       "      <td>0.449776</td>\n",
       "      <td>0.073889</td>\n",
       "      <td>0.065820</td>\n",
       "      <td>0.081152</td>\n",
       "      <td>0.077799</td>\n",
       "      <td>0.064068</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat</th>\n",
       "      <td>0.449776</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.073701</td>\n",
       "      <td>0.065670</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>0.077590</td>\n",
       "      <td>0.063926</td>\n",
       "      <td>0.045669</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car</th>\n",
       "      <td>0.073889</td>\n",
       "      <td>0.073701</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.253965</td>\n",
       "      <td>0.081350</td>\n",
       "      <td>0.077980</td>\n",
       "      <td>0.070453</td>\n",
       "      <td>0.048906</td>\n",
       "      <td>Bicycle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bicycle</th>\n",
       "      <td>0.065820</td>\n",
       "      <td>0.065670</td>\n",
       "      <td>0.253965</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.071675</td>\n",
       "      <td>0.069047</td>\n",
       "      <td>0.063079</td>\n",
       "      <td>0.045235</td>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tree</th>\n",
       "      <td>0.081152</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>0.081350</td>\n",
       "      <td>0.071675</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.249736</td>\n",
       "      <td>0.069602</td>\n",
       "      <td>0.048494</td>\n",
       "      <td>Flower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flower</th>\n",
       "      <td>0.077799</td>\n",
       "      <td>0.077590</td>\n",
       "      <td>0.077980</td>\n",
       "      <td>0.069047</td>\n",
       "      <td>0.249736</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.067121</td>\n",
       "      <td>0.047277</td>\n",
       "      <td>Tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water</th>\n",
       "      <td>0.064068</td>\n",
       "      <td>0.063926</td>\n",
       "      <td>0.070453</td>\n",
       "      <td>0.063079</td>\n",
       "      <td>0.069602</td>\n",
       "      <td>0.067121</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.050328</td>\n",
       "      <td>Car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rainbow</th>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.045669</td>\n",
       "      <td>0.048906</td>\n",
       "      <td>0.045235</td>\n",
       "      <td>0.048494</td>\n",
       "      <td>0.047277</td>\n",
       "      <td>0.050328</td>\n",
       "      <td>inf</td>\n",
       "      <td>Water</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Dog       Cat       Car   Bicycle      Tree    Flower     Water  \\\n",
       "Dog           inf  0.449776  0.073889  0.065820  0.081152  0.077799  0.064068   \n",
       "Cat      0.449776       inf  0.073701  0.065670  0.080924  0.077590  0.063926   \n",
       "Car      0.073889  0.073701       inf  0.253965  0.081350  0.077980  0.070453   \n",
       "Bicycle  0.065820  0.065670  0.253965       inf  0.071675  0.069047  0.063079   \n",
       "Tree     0.081152  0.080924  0.081350  0.071675       inf  0.249736  0.069602   \n",
       "Flower   0.077799  0.077590  0.077980  0.069047  0.249736       inf  0.067121   \n",
       "Water    0.064068  0.063926  0.070453  0.063079  0.069602  0.067121       inf   \n",
       "Rainbow  0.045741  0.045669  0.048906  0.045235  0.048494  0.047277  0.050328   \n",
       "\n",
       "          Rainbow most_similarity  \n",
       "Dog      0.045741             Cat  \n",
       "Cat      0.045669             Dog  \n",
       "Car      0.048906         Bicycle  \n",
       "Bicycle  0.045235             Car  \n",
       "Tree     0.048494          Flower  \n",
       "Flower   0.047277            Tree  \n",
       "Water    0.050328             Car  \n",
       "Rainbow       inf           Water  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(jiang_conrath_similarities , columns=words , index=words ,dtype=\"float32\")\n",
    "df[\"most_similarity\"] = df.apply(lambda row:most_similary_word(row , df.columns) , axis=1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part 9\n",
    "## part 9-a :\n",
    "tokenize and delete stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_tokens = regexp_tokenize(sport_text,\"\\w+\")\n",
    "tech_tokens = regexp_tokenize(tech_text,\"\\w+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 9-b :\n",
    "delete stopwords and single character words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_tokens = [word for word in sport_tokens if len(word)>1 and word.lower() not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_tokens = [word for word in tech_tokens if len(word)>1 and word.lower() not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 9-c:\n",
    "types of texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for sports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types count :  11010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'willingness',\n",
       " 'Coutts',\n",
       " 'initially',\n",
       " 'Weepu',\n",
       " 'Faultless',\n",
       " 'fulfil',\n",
       " 'Defenders',\n",
       " 'inevitable',\n",
       " 'bumps',\n",
       " 'Remy',\n",
       " 'cast',\n",
       " 'flattering',\n",
       " 'Wyatt',\n",
       " 'tame',\n",
       " 'defining',\n",
       " 'attempts',\n",
       " 'Florida',\n",
       " 'contemplating',\n",
       " 'infringements',\n",
       " '19th',\n",
       " 'deceptive',\n",
       " '92',\n",
       " 'covers',\n",
       " 'Burchill',\n",
       " 'death',\n",
       " 'attentions',\n",
       " 'subliminal',\n",
       " 'powered',\n",
       " 'involving',\n",
       " 'sight',\n",
       " 'Arnoud',\n",
       " 'barged',\n",
       " 'Everitt',\n",
       " 'thought',\n",
       " 'enjoyed',\n",
       " 'Derby',\n",
       " 'string',\n",
       " '113',\n",
       " 'Army',\n",
       " 'palmed',\n",
       " 'Woodman',\n",
       " 'lying',\n",
       " 'loans',\n",
       " 'Richie',\n",
       " 'Abebe',\n",
       " 'unveiled',\n",
       " 'pox',\n",
       " 'unforgiving',\n",
       " 'Candela',\n",
       " 'setting',\n",
       " 'Ljubicic',\n",
       " 'Lilley',\n",
       " 'main',\n",
       " 'Emerald',\n",
       " 'World',\n",
       " 'fierce',\n",
       " 'gap',\n",
       " 'Fletcher',\n",
       " '19',\n",
       " 'Japanese',\n",
       " 'finisher',\n",
       " 'Added',\n",
       " 'Julien',\n",
       " 'Ellery',\n",
       " 'Dwain',\n",
       " 'Jaslyn',\n",
       " '66m',\n",
       " 'refute',\n",
       " 'nitrogen',\n",
       " 'shooting',\n",
       " 'Pugh',\n",
       " 'levels',\n",
       " 'derby',\n",
       " 'swearing',\n",
       " 'coming',\n",
       " 'package',\n",
       " 'wits',\n",
       " 'Horgan',\n",
       " 'wall',\n",
       " 'summit',\n",
       " 'memorable',\n",
       " 'Djimi',\n",
       " 'squeezing',\n",
       " 'Dulko',\n",
       " 'Buxton',\n",
       " 'remotely',\n",
       " 'Bosman',\n",
       " 'likes',\n",
       " '184',\n",
       " 'Matches',\n",
       " 'CSA',\n",
       " 'booming',\n",
       " 'bewilderment',\n",
       " 'Wallaby',\n",
       " 'deliberately',\n",
       " 'buffers',\n",
       " 'Neville',\n",
       " 'Ampadu',\n",
       " 'Much',\n",
       " 'Millwall',\n",
       " 'outgoing',\n",
       " 'anybody',\n",
       " 'marked',\n",
       " 'touching',\n",
       " 'made',\n",
       " 'stated',\n",
       " 'sparkling',\n",
       " 'Cherry',\n",
       " 'contemplates',\n",
       " 'friendlies',\n",
       " 'fantastically',\n",
       " 'roused',\n",
       " 'trapped',\n",
       " 'whatsoever',\n",
       " 'regularly',\n",
       " 'Laboratories',\n",
       " 'Jennings',\n",
       " '52',\n",
       " 'Talal',\n",
       " 'shackles',\n",
       " 'Broster',\n",
       " 'lot',\n",
       " '64m',\n",
       " 'late',\n",
       " 'whipped',\n",
       " 'Javier',\n",
       " 'bans',\n",
       " 'Thomas',\n",
       " 'Cicero',\n",
       " 'repeatedly',\n",
       " 'clinic',\n",
       " 'acrobatically',\n",
       " 'proposal',\n",
       " 'stalemate',\n",
       " 'foundation',\n",
       " 'transformed',\n",
       " 'Peters',\n",
       " 'Warbrick',\n",
       " 'Marta',\n",
       " 'sevens',\n",
       " 'somewhat',\n",
       " 'smaller',\n",
       " 'summon',\n",
       " 'loss',\n",
       " 'overseas',\n",
       " 'Zakuani',\n",
       " 'young',\n",
       " 'attacking',\n",
       " 'stance',\n",
       " 'Casey',\n",
       " 'silenced',\n",
       " 'dummy',\n",
       " '147',\n",
       " 'Boumsong',\n",
       " 'eye',\n",
       " 'Sorrell',\n",
       " 'mentality',\n",
       " 'annoyed',\n",
       " 'application',\n",
       " 'touring',\n",
       " 'indictment',\n",
       " 'explore',\n",
       " 'Gnoll',\n",
       " 'epitome',\n",
       " 'Daniela',\n",
       " 'lower',\n",
       " 'daunting',\n",
       " 'Lester',\n",
       " 'Wimbledon',\n",
       " 'deduction',\n",
       " 'unknown',\n",
       " 'Aurelien',\n",
       " 'Nationwide',\n",
       " 'outplays',\n",
       " 'Financially',\n",
       " 'create',\n",
       " 'adage',\n",
       " 'innuendoes',\n",
       " 'blooming',\n",
       " 'Freeland',\n",
       " 'fracas',\n",
       " 'Asagoe',\n",
       " 'Crossley',\n",
       " 'thumped',\n",
       " 'scheduled',\n",
       " 'balls',\n",
       " 'equal',\n",
       " 'Plaisir',\n",
       " 'Matchwinner',\n",
       " 'captured',\n",
       " 'may',\n",
       " 'McConnell',\n",
       " 'Dame',\n",
       " 'Nacho',\n",
       " '73',\n",
       " 'hearts',\n",
       " 'disappointing',\n",
       " 'Independence',\n",
       " 'experimental',\n",
       " 'Cyprus',\n",
       " 'guys',\n",
       " 'Christ',\n",
       " 'withdrawal',\n",
       " 'creaky',\n",
       " 'demand',\n",
       " 'suspension',\n",
       " 'permitted',\n",
       " 'Athletes',\n",
       " 'Traille',\n",
       " 'appearances',\n",
       " '38',\n",
       " '87th',\n",
       " 'Geordan',\n",
       " 'robust',\n",
       " 'agreed',\n",
       " 'killers',\n",
       " 'strike',\n",
       " 'contested',\n",
       " 'Goldstein',\n",
       " 'tapped',\n",
       " 'Owens',\n",
       " 'Reverend',\n",
       " 'ethic',\n",
       " 'Gillick',\n",
       " 'colourful',\n",
       " 'Finally',\n",
       " 'Elsewhere',\n",
       " 'attacks',\n",
       " 'know',\n",
       " 'video',\n",
       " 'reprieve',\n",
       " 'sum',\n",
       " 'Nordt',\n",
       " 'Nihat',\n",
       " 'gather',\n",
       " 'stroll',\n",
       " 'daughter',\n",
       " 'rallying',\n",
       " 'Newport',\n",
       " 'verdict',\n",
       " 'Sculthorpe',\n",
       " 'guessing',\n",
       " 'restored',\n",
       " 'briefly',\n",
       " 'chicken',\n",
       " 'Brien',\n",
       " 'six',\n",
       " 'apparent',\n",
       " 'pass',\n",
       " 'Troncon',\n",
       " 'Spurs',\n",
       " 'anniversary',\n",
       " 'crowning',\n",
       " 'vying',\n",
       " 'choose',\n",
       " 'scarred',\n",
       " 'Rowan',\n",
       " 'concert',\n",
       " 'Queens',\n",
       " 'Scotland',\n",
       " 'volumes',\n",
       " 'giveaway',\n",
       " 'appeals',\n",
       " 'journey',\n",
       " 'admissibility',\n",
       " 'McCormack',\n",
       " 'Euell',\n",
       " 'presentable',\n",
       " '27th',\n",
       " 'gold',\n",
       " 'Bid',\n",
       " 'tactics',\n",
       " 'asI',\n",
       " 'beautiful',\n",
       " 'hoisted',\n",
       " 'protests',\n",
       " 'Forwards',\n",
       " 'fuel',\n",
       " 'misuse',\n",
       " 'linking',\n",
       " 'securing',\n",
       " 'Allister',\n",
       " 'Sylvain',\n",
       " 'define',\n",
       " 'slim',\n",
       " 'beamed',\n",
       " 'Since',\n",
       " '47secs',\n",
       " 'hello',\n",
       " '43',\n",
       " 'standout',\n",
       " 'verge',\n",
       " 'Conchita',\n",
       " 'truth',\n",
       " '84',\n",
       " 'psychology',\n",
       " 'exclusion',\n",
       " 'announcing',\n",
       " 'fairway',\n",
       " 'contaminated',\n",
       " 'corners',\n",
       " 'tends',\n",
       " 'grips',\n",
       " 'Goran',\n",
       " 'brilliance',\n",
       " 'happen',\n",
       " 'meaning',\n",
       " 'harbours',\n",
       " 'crave',\n",
       " '70m',\n",
       " 'aggravating',\n",
       " 'stomach',\n",
       " 'cautious',\n",
       " 'wait',\n",
       " 'Helens',\n",
       " 'Burger',\n",
       " 'comes',\n",
       " 'turbulent',\n",
       " 'celebrate',\n",
       " 'Barkley',\n",
       " 'Waterford',\n",
       " 'exactly',\n",
       " '1946',\n",
       " 'countrymen',\n",
       " 'Feyenoord',\n",
       " 'Certain',\n",
       " 'disgraceful',\n",
       " '50K',\n",
       " 'hockey',\n",
       " 'enjoying',\n",
       " 'Plus',\n",
       " 'tasty',\n",
       " 'McIndoe',\n",
       " 'bright',\n",
       " 'reasonable',\n",
       " 'Londoner',\n",
       " 'Safin',\n",
       " 'Gunnar',\n",
       " 'miserable',\n",
       " 'Navratilova',\n",
       " 'pledged',\n",
       " 'intensive',\n",
       " '94',\n",
       " 'years',\n",
       " 'soared',\n",
       " 'Carr',\n",
       " 'Nikolay',\n",
       " 'Monday',\n",
       " 'unheralded',\n",
       " 'Brownsword',\n",
       " 'victorious',\n",
       " 'downs',\n",
       " 'Two',\n",
       " 'participate',\n",
       " 'reporters',\n",
       " '1990',\n",
       " 'Marvin',\n",
       " 'Bryn',\n",
       " 'table',\n",
       " '1970',\n",
       " 'Paris',\n",
       " 'Luscombe',\n",
       " 'technology',\n",
       " 'unprofessional',\n",
       " 'onus',\n",
       " 'prompting',\n",
       " 'Whatever',\n",
       " 'Warren',\n",
       " 'thrusts',\n",
       " 'competitor',\n",
       " 'Roy',\n",
       " 'Nzame',\n",
       " 'Bernardo',\n",
       " 'Alain',\n",
       " 'deserved',\n",
       " 'Ulster',\n",
       " 'support',\n",
       " 'Crawford',\n",
       " 'Lawn',\n",
       " 'steam',\n",
       " 'surrendered',\n",
       " 'tackling',\n",
       " 'Rowntree',\n",
       " 'guilty',\n",
       " 'sister',\n",
       " 'exits',\n",
       " 'Wolfram',\n",
       " 'cattle',\n",
       " 'Whether',\n",
       " 'hapless',\n",
       " 'legitimately',\n",
       " 'compulsory',\n",
       " 'Defiant',\n",
       " 'genuinely',\n",
       " 'improper',\n",
       " 'Smit',\n",
       " 'Kronberg',\n",
       " 'described',\n",
       " 'shut',\n",
       " 'Buckingham',\n",
       " 'speculate',\n",
       " 'offending',\n",
       " 'remembered',\n",
       " 'Olympiakos',\n",
       " 'Invitational',\n",
       " 'Awards',\n",
       " 'events',\n",
       " 'foods',\n",
       " 'seize',\n",
       " 'Kilbride',\n",
       " 'muscles',\n",
       " 'Rabeni',\n",
       " '69',\n",
       " 'Hierro',\n",
       " 'Staten',\n",
       " 'stroke',\n",
       " 'unbelievable',\n",
       " 'decreed',\n",
       " 'felled',\n",
       " 'imagine',\n",
       " 'exploded',\n",
       " 'notably',\n",
       " 'treating',\n",
       " 'specific',\n",
       " 'wise',\n",
       " 'vertical',\n",
       " 'volleyed',\n",
       " 'claiming',\n",
       " 'overcame',\n",
       " 'extremely',\n",
       " 'north',\n",
       " 'bounced',\n",
       " 'Emilio',\n",
       " 'Achilles',\n",
       " '25m',\n",
       " 'Gaizka',\n",
       " '91m',\n",
       " 'fed',\n",
       " 'issuing',\n",
       " 'Lendl',\n",
       " 'slide',\n",
       " 'Gianluca',\n",
       " 'Fans',\n",
       " 'faith',\n",
       " 'frenetic',\n",
       " 'repaid',\n",
       " 'Argentina',\n",
       " 'en',\n",
       " 'acquainted',\n",
       " '3000m',\n",
       " 'Orr',\n",
       " 'country',\n",
       " 'Gotting',\n",
       " 'Taylor',\n",
       " 'Worse',\n",
       " 'AAA',\n",
       " 'Hernych',\n",
       " 'Kai',\n",
       " 'Wer',\n",
       " 'citizenship',\n",
       " 'Wolverhampton',\n",
       " 'unacceptable',\n",
       " 'affected',\n",
       " 'Wanderers',\n",
       " 'connected',\n",
       " 'lines',\n",
       " 'climax',\n",
       " 'Ibehre',\n",
       " 'Unless',\n",
       " 'scared',\n",
       " 'Meseret',\n",
       " 'magical',\n",
       " 'Keothavong',\n",
       " 'Rugby',\n",
       " 'tears',\n",
       " 'Steinmetz',\n",
       " 'Pierce',\n",
       " 'maintains',\n",
       " 'helped',\n",
       " 'maintain',\n",
       " 'Wright',\n",
       " 'virus',\n",
       " 'Jole',\n",
       " 'sounded',\n",
       " 'snaffled',\n",
       " 'student',\n",
       " 'defend',\n",
       " 'Hinckley',\n",
       " 'subsequent',\n",
       " 'Compatriots',\n",
       " 'newspapers',\n",
       " 'Sar',\n",
       " 'teenage',\n",
       " 'unstoppable',\n",
       " 'relinquish',\n",
       " 'Bozzi',\n",
       " 'Atlantic',\n",
       " 'Surrey',\n",
       " 'madness',\n",
       " 'plum',\n",
       " 'Tranmere',\n",
       " 'crushed',\n",
       " 'century',\n",
       " 'counterproductive',\n",
       " 'Brazilian',\n",
       " 'semi',\n",
       " 'dominate',\n",
       " 'Scandinavia',\n",
       " 'Village',\n",
       " 'Ferreira',\n",
       " 'millions',\n",
       " 'Yann',\n",
       " 'Mutola',\n",
       " 'anything',\n",
       " 'setter',\n",
       " '22secs',\n",
       " '29',\n",
       " 'reopened',\n",
       " 'considered',\n",
       " 'spilling',\n",
       " 'welcoming',\n",
       " 'dividends',\n",
       " 'loosehead',\n",
       " 'Kelly',\n",
       " 'size',\n",
       " 'see',\n",
       " 'bay',\n",
       " 'manoeuvre',\n",
       " 'bottom',\n",
       " 'inner',\n",
       " 'authority',\n",
       " 'Upson',\n",
       " 'Margot',\n",
       " 'Lothian',\n",
       " 'Mulugeta',\n",
       " 'breezes',\n",
       " 'streak',\n",
       " 'ban',\n",
       " '158th',\n",
       " 'clubman',\n",
       " 'knew',\n",
       " 'Flatman',\n",
       " 'Seeds',\n",
       " 'AMRO',\n",
       " 'Bortolami',\n",
       " 'Almagro',\n",
       " 'Alcock',\n",
       " '1905',\n",
       " 'commits',\n",
       " 'amalgamated',\n",
       " 'proved',\n",
       " '2340',\n",
       " 'sapping',\n",
       " 'ages',\n",
       " 'Ricardo',\n",
       " 'bottle',\n",
       " 'cannot',\n",
       " 'gem',\n",
       " 'assessment',\n",
       " 'Macey',\n",
       " 'deserve',\n",
       " 'Almost',\n",
       " 'Causeway',\n",
       " 'cheers',\n",
       " 'height',\n",
       " 'dealt',\n",
       " 'St',\n",
       " 'Yapp',\n",
       " 'mobility',\n",
       " 'UK',\n",
       " 'Welford',\n",
       " 'Tomlinson',\n",
       " 'precaution',\n",
       " 'anouncing',\n",
       " 'partnered',\n",
       " 'Hickie',\n",
       " 'Collier',\n",
       " 'taunting',\n",
       " 'cooled',\n",
       " 'Christmas',\n",
       " 'forthcoming',\n",
       " 'mystified',\n",
       " 'brick',\n",
       " 'Middle',\n",
       " 'Penney',\n",
       " 'showdown',\n",
       " 'Curbishley',\n",
       " 'Inspirational',\n",
       " 'Allyn',\n",
       " 'unit',\n",
       " 'colossus',\n",
       " 'Lehmann',\n",
       " 'coupled',\n",
       " 'sounds',\n",
       " 'exhausted',\n",
       " 'Charleroi',\n",
       " 'Bar',\n",
       " 'criteria',\n",
       " 'Bergh',\n",
       " 'Andrew',\n",
       " 'stole',\n",
       " 'Said',\n",
       " 'prohibited',\n",
       " 'suspicion',\n",
       " 'Aged',\n",
       " '24',\n",
       " 'blood',\n",
       " 'laser',\n",
       " 'discomfort',\n",
       " 'least',\n",
       " 'Veteran',\n",
       " 'note',\n",
       " 'Laulala',\n",
       " 'comment',\n",
       " 'dramatically',\n",
       " 'Aussie',\n",
       " 'Wayne',\n",
       " 'Haim',\n",
       " 'nasty',\n",
       " 'natural',\n",
       " 'reflecion',\n",
       " 'single',\n",
       " 'protest',\n",
       " 'Estonia',\n",
       " '168',\n",
       " 'conceivable',\n",
       " 'Jay',\n",
       " 'Millar',\n",
       " 'Ranieri',\n",
       " 'Live',\n",
       " 'five',\n",
       " 'caught',\n",
       " 'Felix',\n",
       " 'Male',\n",
       " 'thrust',\n",
       " 'Spaniard',\n",
       " '1984',\n",
       " 'disheartened',\n",
       " 'error',\n",
       " 'Hiley',\n",
       " 'Spoon',\n",
       " '1935',\n",
       " 'Desailly',\n",
       " 'Hearts',\n",
       " 'writing',\n",
       " 'humble',\n",
       " 'reversed',\n",
       " 'ignite',\n",
       " 'memory',\n",
       " 'spitting',\n",
       " 'arrangement',\n",
       " 'creates',\n",
       " 'full',\n",
       " 'WTA',\n",
       " 'version',\n",
       " 'grand',\n",
       " 'Coe',\n",
       " 'nail',\n",
       " 'McGreal',\n",
       " 'reeled',\n",
       " 'tend',\n",
       " 'sprints',\n",
       " 'require',\n",
       " 'Within',\n",
       " 'city',\n",
       " 'winless',\n",
       " 'Ok',\n",
       " 'conditions',\n",
       " 'balance',\n",
       " 'Janine',\n",
       " 'also',\n",
       " 'selectors',\n",
       " 'journalists',\n",
       " 'Nixon',\n",
       " 'Performance',\n",
       " 'smother',\n",
       " 'Munich',\n",
       " 'Mandula',\n",
       " 'Herve',\n",
       " 'animosity',\n",
       " 'completely',\n",
       " 'compact',\n",
       " 'accumulated',\n",
       " 'Karol',\n",
       " 'rumbled',\n",
       " 'Rafael',\n",
       " 'Panagopoulos',\n",
       " 'crosscourt',\n",
       " 'women',\n",
       " 'Bemand',\n",
       " 'yes',\n",
       " 'Louise',\n",
       " 'Courier',\n",
       " 'Hundreds',\n",
       " 'Gallas',\n",
       " 'pretty',\n",
       " 'courses',\n",
       " 'daily',\n",
       " 'verification',\n",
       " 'gear',\n",
       " 'arguments',\n",
       " 'Frederic',\n",
       " 'Warne',\n",
       " 'Seti',\n",
       " 'Dempsey',\n",
       " 'awarded',\n",
       " 'lose',\n",
       " 'clashing',\n",
       " 'wife',\n",
       " 'Childs',\n",
       " 'soon',\n",
       " 'tackled',\n",
       " 'breakthrough',\n",
       " 'concentration',\n",
       " 'Challenger',\n",
       " 'Tardelli',\n",
       " 'takeover',\n",
       " 'hooker',\n",
       " 'shrugged',\n",
       " 'irreplaceable',\n",
       " 'Howarth',\n",
       " 'Sudan',\n",
       " 'sustaining',\n",
       " 'Tom',\n",
       " 'relished',\n",
       " 'Powergen',\n",
       " 'Cameron',\n",
       " 'lured',\n",
       " 'immaculate',\n",
       " 'Fabregas',\n",
       " 'Hong',\n",
       " '63secs',\n",
       " 'Cockerill',\n",
       " 'Farrell',\n",
       " 'Carwyn',\n",
       " 'hold',\n",
       " 'Sprinter',\n",
       " 'Upping',\n",
       " 'distinctive',\n",
       " 'Yiannis',\n",
       " 'Yeading',\n",
       " 'Hashemian',\n",
       " 'abroad',\n",
       " 'reacted',\n",
       " 'sets',\n",
       " 'Nevis',\n",
       " 'Gillespie',\n",
       " 'inspiration',\n",
       " 'close',\n",
       " 'range',\n",
       " 'admit',\n",
       " 'urban',\n",
       " 'Stoddart',\n",
       " 'paper',\n",
       " 'hinge',\n",
       " 'Toulouse',\n",
       " 'Bartholomeusz',\n",
       " 'Makaji',\n",
       " 'rare',\n",
       " '66',\n",
       " 'territory',\n",
       " 'Sav',\n",
       " 'dependable',\n",
       " '093',\n",
       " 'downward',\n",
       " 'deciding',\n",
       " 'elements',\n",
       " 'start',\n",
       " 'upbeat',\n",
       " 'inadvertently',\n",
       " 'calf',\n",
       " 'slower',\n",
       " 'flared',\n",
       " 'Ever',\n",
       " 'silly',\n",
       " 'Japan',\n",
       " 'Pognon',\n",
       " 'destination',\n",
       " 'Luczak',\n",
       " 'partnership',\n",
       " 'cheering',\n",
       " 'external',\n",
       " 'ahead',\n",
       " 'Ricky',\n",
       " 'slowly',\n",
       " '90m',\n",
       " 'ever',\n",
       " 'heats',\n",
       " 'convulsed',\n",
       " 'risen',\n",
       " 'Ruddock',\n",
       " 'Shrewsbury',\n",
       " 'enthralling',\n",
       " 'Toshack',\n",
       " 'progressed',\n",
       " 'discipline',\n",
       " 'survives',\n",
       " 'cancelled',\n",
       " '47',\n",
       " 'reflection',\n",
       " 'gloss',\n",
       " 'Caledonian',\n",
       " 'Mladenovic',\n",
       " 'Charles',\n",
       " 'dry',\n",
       " 'hundreth',\n",
       " 'marketed',\n",
       " 'sprain',\n",
       " 'agreeing',\n",
       " 'courters',\n",
       " 'sweeter',\n",
       " 'Fifa',\n",
       " 'Chorley',\n",
       " 'ABN',\n",
       " 'Strong',\n",
       " 'Shaka',\n",
       " 'Federer',\n",
       " 'fear',\n",
       " 'risked',\n",
       " 'consult',\n",
       " 'command',\n",
       " 'compromise',\n",
       " 'rubbers',\n",
       " 'bodied',\n",
       " 'breaking',\n",
       " 'normal',\n",
       " 'Colin',\n",
       " 'PSG',\n",
       " 'rid',\n",
       " 'exceptional',\n",
       " 'cordon',\n",
       " 'Sheffield',\n",
       " 'apply',\n",
       " 'echoed',\n",
       " 'Anyone',\n",
       " 'hung',\n",
       " 'nailed',\n",
       " 'suck',\n",
       " 'Meersseman',\n",
       " 'overshadow',\n",
       " 'Shanklin',\n",
       " 'Pape',\n",
       " 'steer',\n",
       " 'unjustified',\n",
       " 'deflected',\n",
       " 'shortly',\n",
       " 'repeating',\n",
       " 'quicker',\n",
       " 'Korea',\n",
       " 'Grecians',\n",
       " 'matched',\n",
       " 'boardroom',\n",
       " 'Raiwalui',\n",
       " 'Claudio',\n",
       " 'dinked',\n",
       " 'Pieters',\n",
       " 'website',\n",
       " 'mins',\n",
       " 'peacemakers',\n",
       " 'Swaziland',\n",
       " '17m',\n",
       " 'giddy',\n",
       " 'walk',\n",
       " 'published',\n",
       " 'look',\n",
       " 'Haystead',\n",
       " 'Leander',\n",
       " 'adds',\n",
       " 'correctly',\n",
       " 'statements',\n",
       " 'lining',\n",
       " 'WINNERS',\n",
       " 'Torres',\n",
       " 'departed',\n",
       " 'Valleys',\n",
       " 'incident',\n",
       " 'hurts',\n",
       " 'Callam',\n",
       " 'parry',\n",
       " '1100',\n",
       " 'accusations',\n",
       " 'Zoe',\n",
       " 'phenomenal',\n",
       " 'IRFU',\n",
       " 'gargantuan',\n",
       " 'fallen',\n",
       " 'Jankovic',\n",
       " 'la',\n",
       " 'punishment',\n",
       " 'Slovakian',\n",
       " 'Russians',\n",
       " 'holds',\n",
       " 'pulling',\n",
       " 'minutes',\n",
       " 'refrained',\n",
       " '79th',\n",
       " 'Ovett',\n",
       " 'reunited',\n",
       " 'court',\n",
       " 'last',\n",
       " 'Schwarzer',\n",
       " 'Selection',\n",
       " 'Burns',\n",
       " 'tended',\n",
       " 'vacated',\n",
       " 'changed',\n",
       " 'Taniesha',\n",
       " 'manages',\n",
       " 'carve',\n",
       " 'partisan',\n",
       " 'fervent',\n",
       " 'arranged',\n",
       " 'Llewellyn',\n",
       " 'Valbon',\n",
       " 'Henman',\n",
       " 'poor',\n",
       " 'Franca',\n",
       " 'Greg',\n",
       " 'signalled',\n",
       " 'could',\n",
       " 'Zinedine',\n",
       " 'fifth',\n",
       " 'Alessandro',\n",
       " 'horror',\n",
       " 'scratch',\n",
       " 'Relay',\n",
       " 'Thaliand',\n",
       " 'Lund',\n",
       " 'notes',\n",
       " 'Director',\n",
       " 'manager',\n",
       " 'Reina',\n",
       " 'Squad',\n",
       " 'required',\n",
       " 'bruised',\n",
       " 'Denis',\n",
       " 'permanent',\n",
       " 'Zealand',\n",
       " 'widely',\n",
       " 'Long',\n",
       " 'search',\n",
       " 'Manchester',\n",
       " 'chanting',\n",
       " 'indeed',\n",
       " 'seeks',\n",
       " 'stating',\n",
       " 'written',\n",
       " 'somewhere',\n",
       " 'shock',\n",
       " 'concept',\n",
       " 'Lupoli',\n",
       " 'smiles',\n",
       " 'upset',\n",
       " 'indoor',\n",
       " 'proceedings',\n",
       " 'experience',\n",
       " 'image',\n",
       " '117th',\n",
       " 'innate',\n",
       " 'racket',\n",
       " 'integrating',\n",
       " 'McCall',\n",
       " 'Fabiola',\n",
       " 'responded',\n",
       " 'lonely',\n",
       " 'strongest',\n",
       " 'Christophe',\n",
       " 'Miresmaeili',\n",
       " '350m',\n",
       " 'pounced',\n",
       " 'Lab',\n",
       " 'devoid',\n",
       " 'Italy',\n",
       " 'permit',\n",
       " 'Everest',\n",
       " 'Olympics',\n",
       " 'unruffled',\n",
       " 'explosive',\n",
       " 'Marat',\n",
       " 'psychologist',\n",
       " 'experiences',\n",
       " 'monotony',\n",
       " 'pendulum',\n",
       " 'Jo',\n",
       " 'sacked',\n",
       " 'Vinci',\n",
       " 'aggressive',\n",
       " 'Rae',\n",
       " 'cheques',\n",
       " 'Twenty',\n",
       " 'languages',\n",
       " 'Bernabeu',\n",
       " 'performances',\n",
       " 'gloomier',\n",
       " 'Ferencvaros',\n",
       " 'Murphy',\n",
       " '1908',\n",
       " 'convertible',\n",
       " 'lighting',\n",
       " 'sidefooted',\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sport_types = set(sport_tokens)\n",
    "print(\"types count : \" , len(sport_types))\n",
    "sport_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for techs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types count :  13404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'willingness',\n",
       " 'initially',\n",
       " 'computer',\n",
       " 'blockbuster',\n",
       " 'flood',\n",
       " 'recognising',\n",
       " 'fulfil',\n",
       " 'Cell',\n",
       " 'Defenders',\n",
       " 'bombards',\n",
       " 'palpable',\n",
       " 'Robotic',\n",
       " 'inevitable',\n",
       " 'adverts',\n",
       " 'invasive',\n",
       " 'organization',\n",
       " 'cast',\n",
       " 'investigates',\n",
       " 'defining',\n",
       " 'amputated',\n",
       " 'attempts',\n",
       " 'inherently',\n",
       " 'ers',\n",
       " 'Property',\n",
       " 'outputs',\n",
       " 'Guido',\n",
       " 'Harvard',\n",
       " 'infringements',\n",
       " '19th',\n",
       " 'deceptive',\n",
       " '92',\n",
       " 'Wanadoo',\n",
       " 'conjoined',\n",
       " 'covers',\n",
       " 'backup',\n",
       " 'hundreds',\n",
       " 'Distributors',\n",
       " 'death',\n",
       " 'sight',\n",
       " 'involving',\n",
       " 'interacted',\n",
       " 'Commenting',\n",
       " 'channel',\n",
       " 'powered',\n",
       " 'boils',\n",
       " 'computing',\n",
       " 'debuting',\n",
       " 'seafarer',\n",
       " 'wholesale',\n",
       " 'thought',\n",
       " 'Podcasting',\n",
       " 'Flaunting',\n",
       " 'contractor',\n",
       " 'Army',\n",
       " 'Derby',\n",
       " 'string',\n",
       " 'enjoyed',\n",
       " 'Barely',\n",
       " 'magnificence',\n",
       " 'Farmers',\n",
       " 'hangar',\n",
       " 'DTI',\n",
       " 'pls',\n",
       " 'lawsuits',\n",
       " 'Experienced',\n",
       " 'lying',\n",
       " 'rotating',\n",
       " 'drumming',\n",
       " 'freed',\n",
       " 'unveiled',\n",
       " 'shakes',\n",
       " 'inkjet',\n",
       " 'setting',\n",
       " 'requiring',\n",
       " 'Kilgore',\n",
       " 'HDMI',\n",
       " 'main',\n",
       " 'Venturas',\n",
       " 'World',\n",
       " 'overtaking',\n",
       " 'gap',\n",
       " 'fierce',\n",
       " 'focal',\n",
       " 'remnants',\n",
       " 'subtleties',\n",
       " '19',\n",
       " 'Japanese',\n",
       " 'interfaces',\n",
       " 'ascending',\n",
       " 'lending',\n",
       " 'mall',\n",
       " 'queuing',\n",
       " 'NHTCU',\n",
       " 'unveils',\n",
       " 'SpeechWorks',\n",
       " 'shooting',\n",
       " 'levels',\n",
       " 'robotiquette',\n",
       " 'coming',\n",
       " 'package',\n",
       " 'fat',\n",
       " 'wits',\n",
       " 'laughable',\n",
       " 'refund',\n",
       " 'wall',\n",
       " 'summit',\n",
       " 'memorable',\n",
       " 'Scout',\n",
       " 'alot',\n",
       " 'Computer',\n",
       " 'remotely',\n",
       " 'dial',\n",
       " 'squeezing',\n",
       " 'likes',\n",
       " 'compatibility',\n",
       " 'marine',\n",
       " 'indefinitely',\n",
       " 'CSA',\n",
       " 'deeply',\n",
       " 'deliberately',\n",
       " 'techno',\n",
       " 'infancy',\n",
       " 'Land',\n",
       " 'coat',\n",
       " 'networked',\n",
       " 'audited',\n",
       " 'Denial',\n",
       " 'Much',\n",
       " 'Lateral',\n",
       " 'outgoing',\n",
       " 'lazy',\n",
       " 'charts',\n",
       " 'marked',\n",
       " 'anybody',\n",
       " 'Demographics',\n",
       " 'touching',\n",
       " 'maturing',\n",
       " 'muddied',\n",
       " 'cam',\n",
       " 'habits',\n",
       " 'made',\n",
       " 'stated',\n",
       " 'hint',\n",
       " 'Minidisc',\n",
       " 'fantastically',\n",
       " 'strangely',\n",
       " 'consultation',\n",
       " 'Warner',\n",
       " 'reliability',\n",
       " 'Mediametrie',\n",
       " 'galleries',\n",
       " 'trapped',\n",
       " 'Dominican',\n",
       " 'Fanlo',\n",
       " 'regularly',\n",
       " 'Laboratories',\n",
       " '52',\n",
       " 'Turismo',\n",
       " 'demons',\n",
       " 'astonishing',\n",
       " 'deteriorated',\n",
       " 'lot',\n",
       " 'boundaries',\n",
       " 'collaborators',\n",
       " 'bans',\n",
       " 'late',\n",
       " 'categorise',\n",
       " 'Thomas',\n",
       " 'repeatedly',\n",
       " 'clinic',\n",
       " 'Hein',\n",
       " 'hidden',\n",
       " 'proposal',\n",
       " 'foundation',\n",
       " 'transformed',\n",
       " 'ingots',\n",
       " 'somewhat',\n",
       " 'Entropia',\n",
       " 'smaller',\n",
       " 'Pictochat',\n",
       " 'School',\n",
       " 'encompasses',\n",
       " 'cordless',\n",
       " 'eminently',\n",
       " 'loss',\n",
       " 'overseas',\n",
       " 'nuisance',\n",
       " '2G',\n",
       " '190',\n",
       " 'THX',\n",
       " 'young',\n",
       " 'unprotected',\n",
       " 'attacking',\n",
       " 'lessens',\n",
       " 'Whistler',\n",
       " 'eye',\n",
       " 'sheep',\n",
       " 'plateau',\n",
       " 'application',\n",
       " 'Gawker',\n",
       " '20s',\n",
       " 'cos',\n",
       " 'explore',\n",
       " 'suprnova',\n",
       " 'hacks',\n",
       " 'lower',\n",
       " 'Intertrust',\n",
       " 'Hicksdesign',\n",
       " 'Competitors',\n",
       " 'Feel',\n",
       " 'Minnesota',\n",
       " 'purposefully',\n",
       " 'unknown',\n",
       " 'Governor',\n",
       " 'MessageLabs',\n",
       " 'context',\n",
       " 'immediacy',\n",
       " 'create',\n",
       " 'Monsters',\n",
       " 'Democrat',\n",
       " 'Concerns',\n",
       " 'participating',\n",
       " 'ordinating',\n",
       " 'significance',\n",
       " 'scheduled',\n",
       " 'equal',\n",
       " 'Partners',\n",
       " 'captured',\n",
       " 'invent',\n",
       " 'may',\n",
       " 'webcasts',\n",
       " 'Redesigning',\n",
       " 'Carly',\n",
       " 'Gillmor',\n",
       " '73',\n",
       " 'stored',\n",
       " 'spreading',\n",
       " 'navigating',\n",
       " 'hearts',\n",
       " 'Dolby',\n",
       " 'disappointing',\n",
       " 'experimental',\n",
       " 'persuading',\n",
       " 'bulky',\n",
       " 'demand',\n",
       " 'engineers',\n",
       " 'curry',\n",
       " 'portions',\n",
       " 'Gormenghast',\n",
       " 'Video',\n",
       " 'permitted',\n",
       " 'suspension',\n",
       " 'Regent',\n",
       " 'BSA',\n",
       " 'measurement',\n",
       " 'Search',\n",
       " '38',\n",
       " 'retailers',\n",
       " 'sic',\n",
       " 'waning',\n",
       " 'cannon',\n",
       " 'robust',\n",
       " 'agreed',\n",
       " 'ethnicities',\n",
       " 'contested',\n",
       " 'compressed',\n",
       " 'strike',\n",
       " 'karaoke',\n",
       " 'Aichi',\n",
       " 'Goldstein',\n",
       " 'teenagers',\n",
       " 'Heavy',\n",
       " 'document',\n",
       " 'materialists',\n",
       " 'colourful',\n",
       " 'concerted',\n",
       " 'Macklin',\n",
       " 'mercury',\n",
       " 'volume',\n",
       " 'Elsewhere',\n",
       " 'attacks',\n",
       " 'know',\n",
       " '133',\n",
       " 'video',\n",
       " 'indexes',\n",
       " 'sum',\n",
       " 'behold',\n",
       " 'gather',\n",
       " 'Sims',\n",
       " 'daughter',\n",
       " 'liner',\n",
       " 'Brydon',\n",
       " 'identity',\n",
       " 'Current',\n",
       " 'restored',\n",
       " 'briefly',\n",
       " 'six',\n",
       " 'apparent',\n",
       " 'gloriously',\n",
       " 'pass',\n",
       " 'addiction',\n",
       " 'Spurs',\n",
       " 'anniversary',\n",
       " 'Premium',\n",
       " 'choose',\n",
       " 'vying',\n",
       " 'outbreaks',\n",
       " 'bills',\n",
       " 'plunders',\n",
       " 'territories',\n",
       " 'fluff',\n",
       " 'distant',\n",
       " 'doorways',\n",
       " 'concert',\n",
       " 'Scotland',\n",
       " 'studios',\n",
       " 'restoration',\n",
       " 'volumes',\n",
       " 'appeals',\n",
       " 'activity',\n",
       " 'Souter',\n",
       " 'thinner',\n",
       " 'sneak',\n",
       " 'journey',\n",
       " 'notoriously',\n",
       " '42in',\n",
       " 'cooperation',\n",
       " 'employers',\n",
       " 'functions',\n",
       " 'curb',\n",
       " 'Hanau',\n",
       " 'gold',\n",
       " 'tactics',\n",
       " 'Bejewelled',\n",
       " 'beautiful',\n",
       " 'EPA',\n",
       " 'Theriault',\n",
       " 'universe',\n",
       " 'Robotics',\n",
       " 'apple',\n",
       " 'sequential',\n",
       " 'fuel',\n",
       " 'misuse',\n",
       " 'Nat',\n",
       " 'gathers',\n",
       " 'linking',\n",
       " 'disquiet',\n",
       " 'Future',\n",
       " 'define',\n",
       " 'PEDs',\n",
       " 'Workmate',\n",
       " 'Client',\n",
       " 'Finn',\n",
       " 'tribes',\n",
       " 'beamed',\n",
       " 'Since',\n",
       " 'Careless',\n",
       " 'themed',\n",
       " 'strides',\n",
       " 'SP2',\n",
       " '2009',\n",
       " 'streets',\n",
       " 'ratified',\n",
       " 'Eminem',\n",
       " '43',\n",
       " 'verge',\n",
       " 'gallery',\n",
       " 'describe',\n",
       " 'truth',\n",
       " 'thirties',\n",
       " '84',\n",
       " 'terminals',\n",
       " 'psychology',\n",
       " 'Supercomputers',\n",
       " 'sucked',\n",
       " 'exclusion',\n",
       " 'Renaissance',\n",
       " 'limitless',\n",
       " '3D',\n",
       " 'announcing',\n",
       " 'Digest',\n",
       " 'custom',\n",
       " 'farmer',\n",
       " 'corners',\n",
       " 'farming',\n",
       " 'extensions',\n",
       " 'Cannes',\n",
       " 'Kbps',\n",
       " 'buffs',\n",
       " 'tends',\n",
       " 'recoup',\n",
       " 'grips',\n",
       " 'happen',\n",
       " 'Linux',\n",
       " 'Butcher',\n",
       " 'meaning',\n",
       " 'harbours',\n",
       " 'crave',\n",
       " 'dioxide',\n",
       " 'aggravating',\n",
       " 'wait',\n",
       " 'cultural',\n",
       " 'cautious',\n",
       " 'adaptors',\n",
       " 'converged',\n",
       " 'unfeasible',\n",
       " 'physics',\n",
       " 'Hardware',\n",
       " 'comes',\n",
       " 'makers',\n",
       " 'exactly',\n",
       " 'entrepreneur',\n",
       " 'transition',\n",
       " 'letters',\n",
       " 'Carpenter',\n",
       " 'Macs',\n",
       " 'virtual',\n",
       " 'manufacturers',\n",
       " 'hockey',\n",
       " 'Plus',\n",
       " 'enjoying',\n",
       " 'pitfalls',\n",
       " 'Swarm',\n",
       " 'lasers',\n",
       " 'Express',\n",
       " 'bright',\n",
       " 'reasonable',\n",
       " 'inadequate',\n",
       " 'adviser',\n",
       " 'criminality',\n",
       " 'MTV',\n",
       " 'misdirection',\n",
       " 'pledged',\n",
       " 'Doling',\n",
       " 'intensive',\n",
       " '94',\n",
       " 'Traditionally',\n",
       " 'sleepwalk',\n",
       " 'FURIOUS',\n",
       " '508m',\n",
       " 'accelerating',\n",
       " 'iBand',\n",
       " 'years',\n",
       " 'DTT',\n",
       " 'Gothamist',\n",
       " 'Disable',\n",
       " 'Avenue',\n",
       " 'Carr',\n",
       " 'Monday',\n",
       " 'gov',\n",
       " 'jails',\n",
       " 'downs',\n",
       " 'Two',\n",
       " 'participate',\n",
       " 'reporters',\n",
       " 'MMO2',\n",
       " 'projects',\n",
       " 'Paris',\n",
       " 'technology',\n",
       " 'Flic',\n",
       " 'Bertie',\n",
       " 'prompting',\n",
       " 'approve',\n",
       " 'Whatever',\n",
       " 'Warren',\n",
       " 'derived',\n",
       " 'stills',\n",
       " 'Georgeta',\n",
       " 'competitor',\n",
       " 'models',\n",
       " 'BYODKM',\n",
       " 'fascination',\n",
       " 'impeded',\n",
       " 'scripted',\n",
       " 'viewfinders',\n",
       " 'consumers',\n",
       " 'Roy',\n",
       " 'Alain',\n",
       " 'deserved',\n",
       " 'trumpeted',\n",
       " 'support',\n",
       " 'Twentieth',\n",
       " 'paint',\n",
       " 'Trend',\n",
       " 'obsessively',\n",
       " 'sharers',\n",
       " 'Jarek',\n",
       " 'MBlox',\n",
       " 'DDoS',\n",
       " 'stealth',\n",
       " 'Nan',\n",
       " 'untapped',\n",
       " 'guilty',\n",
       " 'Dayton',\n",
       " 'Satoru',\n",
       " 'pump',\n",
       " 'sister',\n",
       " 'scenarios',\n",
       " 'biocidal',\n",
       " 'backtrack',\n",
       " 'pilot',\n",
       " 'Whether',\n",
       " 'Twelve',\n",
       " 'controlling',\n",
       " 'Name',\n",
       " 'Cracknell',\n",
       " 'designed',\n",
       " 'Existence',\n",
       " 'fonts',\n",
       " 'teach',\n",
       " 'improper',\n",
       " 'gong',\n",
       " 'diatribes',\n",
       " 'described',\n",
       " 'faulty',\n",
       " 'shut',\n",
       " 'Film',\n",
       " 'Buckingham',\n",
       " 'negotiating',\n",
       " 'speculate',\n",
       " 'paramount',\n",
       " 'Broadcasters',\n",
       " 'Go',\n",
       " 'Huaral',\n",
       " 'primetime',\n",
       " 'grateful',\n",
       " 'Awards',\n",
       " 'innocently',\n",
       " 'Article',\n",
       " 'events',\n",
       " 'seize',\n",
       " 'Resources',\n",
       " '028',\n",
       " 'muscles',\n",
       " '69',\n",
       " 'stroke',\n",
       " 'Jordanian',\n",
       " 'pandas',\n",
       " 'unbelievable',\n",
       " 'openness',\n",
       " 'magazines',\n",
       " 'compiles',\n",
       " 'classy',\n",
       " 'AU',\n",
       " 'R800',\n",
       " 'imagine',\n",
       " 'exploded',\n",
       " 'playlists',\n",
       " 'notably',\n",
       " 'cosmetics',\n",
       " 'specific',\n",
       " 'certificate',\n",
       " 'inflated',\n",
       " 'readers',\n",
       " 'Bicycle',\n",
       " 'wise',\n",
       " 'claiming',\n",
       " 'extremely',\n",
       " 'north',\n",
       " 'democratising',\n",
       " 'MP3s',\n",
       " 'bobs',\n",
       " 'codename',\n",
       " 'imparting',\n",
       " '25m',\n",
       " 'Rendering',\n",
       " 'err',\n",
       " 'Unbundling',\n",
       " 'cancer',\n",
       " 'purrs',\n",
       " 'slinkier',\n",
       " 'fed',\n",
       " 'issuing',\n",
       " 'Cells',\n",
       " 'corrective',\n",
       " 'slide',\n",
       " 'spawned',\n",
       " 'Fans',\n",
       " 'messaging',\n",
       " 'clues',\n",
       " 'kettle',\n",
       " 'frenetic',\n",
       " 'Argentina',\n",
       " 'en',\n",
       " 'desolate',\n",
       " 'unlock',\n",
       " 'firewalls',\n",
       " 'country',\n",
       " 'surreptitious',\n",
       " 'stockpile',\n",
       " 'Afghanistan',\n",
       " 'Taylor',\n",
       " 'AAA',\n",
       " 'Morpheme',\n",
       " 'TI',\n",
       " 'endedness',\n",
       " 'sounding',\n",
       " 'routing',\n",
       " 'Tata',\n",
       " 'Cable',\n",
       " 'Traditional',\n",
       " 'migrate',\n",
       " 'underwater',\n",
       " 'unacceptable',\n",
       " 'affected',\n",
       " 'connected',\n",
       " 'lines',\n",
       " 'climax',\n",
       " 'Unless',\n",
       " 'scared',\n",
       " 'magical',\n",
       " 'diversionary',\n",
       " 'McRae',\n",
       " 'index',\n",
       " 'maintains',\n",
       " 'helped',\n",
       " 'maintain',\n",
       " 'virus',\n",
       " 'Wright',\n",
       " 'Dell',\n",
       " 'sliced',\n",
       " 'student',\n",
       " 'aired',\n",
       " 'forewarning',\n",
       " 'drivers',\n",
       " 'amplifies',\n",
       " 'defend',\n",
       " 'width',\n",
       " 'subsequent',\n",
       " 'untouched',\n",
       " 'newspapers',\n",
       " 'Trains',\n",
       " 'eerie',\n",
       " 'Surrey',\n",
       " 'teenage',\n",
       " 'assist',\n",
       " 'Dautenhahn',\n",
       " 'Manipulating',\n",
       " 'plum',\n",
       " 'century',\n",
       " 'insurance',\n",
       " 'Brazilian',\n",
       " 'semi',\n",
       " 'wired',\n",
       " 'dominate',\n",
       " 'millions',\n",
       " 'statistical',\n",
       " 'southwest',\n",
       " 'shuttle',\n",
       " 'anything',\n",
       " 'stifle',\n",
       " 'Band',\n",
       " 'enthusiasts',\n",
       " 'Mellon',\n",
       " 'electoral',\n",
       " 'Soda',\n",
       " '29',\n",
       " 'Shift',\n",
       " 'considered',\n",
       " 'spilling',\n",
       " 'dissuading',\n",
       " 'variants',\n",
       " 'Driving',\n",
       " 'postal',\n",
       " 'Make',\n",
       " 'backsliding',\n",
       " 'Footage',\n",
       " 'cafes',\n",
       " 'Kelly',\n",
       " 'wheeled',\n",
       " 'Bae',\n",
       " 'size',\n",
       " 'see',\n",
       " 'bay',\n",
       " 'Towards',\n",
       " '1bn',\n",
       " 'Biodiversity',\n",
       " 'laptop',\n",
       " 'pixel',\n",
       " 'bottom',\n",
       " 'invasiveness',\n",
       " 'Ultimate',\n",
       " 'terabytes',\n",
       " 'authority',\n",
       " 'Houlihan',\n",
       " 'GNER',\n",
       " 'breezes',\n",
       " 'ban',\n",
       " 'knew',\n",
       " 'Carderplanet',\n",
       " 'inherent',\n",
       " 'bots',\n",
       " 'purged',\n",
       " 'Montrose',\n",
       " 'CM',\n",
       " 'yahoo',\n",
       " 'keypad',\n",
       " 'pairs',\n",
       " 'proved',\n",
       " 'ages',\n",
       " 'texture',\n",
       " 'bottle',\n",
       " 'deserts',\n",
       " 'multilingual',\n",
       " 'cannot',\n",
       " 'analysed',\n",
       " 'assessment',\n",
       " 'Almost',\n",
       " 'deserve',\n",
       " 'Apocalypse',\n",
       " 'breakfast',\n",
       " 'judgment',\n",
       " 'Hi',\n",
       " 'height',\n",
       " '425',\n",
       " 'dealt',\n",
       " 'leaps',\n",
       " 'St',\n",
       " 'slant',\n",
       " 'Harker',\n",
       " 'mobility',\n",
       " 'UK',\n",
       " 'diapers',\n",
       " 'viable',\n",
       " 'theoretical',\n",
       " 'partnered',\n",
       " 'chronometer',\n",
       " 'Angela',\n",
       " 'Christmas',\n",
       " 'port',\n",
       " 'forthcoming',\n",
       " 'pinpoint',\n",
       " 'filmed',\n",
       " 'Middle',\n",
       " '700',\n",
       " 'molecules',\n",
       " 'bait',\n",
       " 'Affairs',\n",
       " 'unit',\n",
       " 'insider',\n",
       " 'wowed',\n",
       " 'sounds',\n",
       " 'coupled',\n",
       " 'criteria',\n",
       " 'franchise',\n",
       " 'Civilisation',\n",
       " 'Andrew',\n",
       " 'sims',\n",
       " 'stole',\n",
       " 'unwanted',\n",
       " 'Broadcasting',\n",
       " 'Said',\n",
       " 'defined',\n",
       " 'prohibited',\n",
       " 'Andreas',\n",
       " 'suspicion',\n",
       " 'drain',\n",
       " 'mums',\n",
       " 'parliamentary',\n",
       " 'tinker',\n",
       " '24',\n",
       " 'McFarlane',\n",
       " 'laser',\n",
       " 'Yankee',\n",
       " 'installations',\n",
       " 'least',\n",
       " 'Horiuchi',\n",
       " 'logistical',\n",
       " 'note',\n",
       " 'Fraud',\n",
       " 'Veteran',\n",
       " 'comment',\n",
       " 'biscuit',\n",
       " 'dramatically',\n",
       " 'cabinets',\n",
       " 'Nintendo',\n",
       " 'nasty',\n",
       " 'natural',\n",
       " 'single',\n",
       " 'iconic',\n",
       " 'Sonaptic',\n",
       " 'protest',\n",
       " 'Estonia',\n",
       " 'discarded',\n",
       " 'anonymity',\n",
       " 'paved',\n",
       " 'Jay',\n",
       " 'soaps',\n",
       " 'Chassis',\n",
       " 'Tiscali',\n",
       " 'seeping',\n",
       " 'Live',\n",
       " 'Paypal',\n",
       " 'five',\n",
       " 'Ames',\n",
       " 'inspire',\n",
       " 'Desperate',\n",
       " 'caught',\n",
       " 'tourism',\n",
       " 'thrust',\n",
       " 'Spaniard',\n",
       " '1984',\n",
       " 'limelight',\n",
       " 'bash',\n",
       " 'ranged',\n",
       " 'retailer',\n",
       " 'error',\n",
       " 'Tulip',\n",
       " 'fields',\n",
       " 'sleek',\n",
       " 'admitting',\n",
       " 'writing',\n",
       " 'humble',\n",
       " 'ignite',\n",
       " 'memory',\n",
       " 'Chapter',\n",
       " 'vocal',\n",
       " 'arrangement',\n",
       " 'creates',\n",
       " 'spitting',\n",
       " 'contributor',\n",
       " 'full',\n",
       " 'receivers',\n",
       " 'Steam',\n",
       " 'version',\n",
       " 'NEXT',\n",
       " 'grand',\n",
       " 'movie',\n",
       " 'tend',\n",
       " 'require',\n",
       " 'Within',\n",
       " 'city',\n",
       " 'profitable',\n",
       " 'poster',\n",
       " 'conditions',\n",
       " 'balance',\n",
       " 'psychoacoustics',\n",
       " 'also',\n",
       " 'Rainbow',\n",
       " 'Gangster',\n",
       " 'journalists',\n",
       " 'Islamic',\n",
       " 'Performance',\n",
       " 'IPods',\n",
       " 'Munich',\n",
       " 'completely',\n",
       " 'compact',\n",
       " 'crackdown',\n",
       " 'Results',\n",
       " 'Patents',\n",
       " 'draining',\n",
       " 'women',\n",
       " 'schemes',\n",
       " 'Hundreds',\n",
       " 'yes',\n",
       " 'pretty',\n",
       " 'courses',\n",
       " 'daily',\n",
       " 'checking',\n",
       " 'Robot',\n",
       " 'gear',\n",
       " 'utilise',\n",
       " 'jailed',\n",
       " 'arguments',\n",
       " 'producer',\n",
       " 'Seti',\n",
       " 'Index',\n",
       " 'ownership',\n",
       " 'gigapixel',\n",
       " 'snowballed',\n",
       " 'Dempsey',\n",
       " 'awarded',\n",
       " 'lose',\n",
       " 'hobble',\n",
       " 'neatly',\n",
       " 'wife',\n",
       " 'zombie',\n",
       " 'soon',\n",
       " 'tackled',\n",
       " 'breakthrough',\n",
       " 'takeover',\n",
       " 'gateway',\n",
       " 'Bodleian',\n",
       " 'watchable',\n",
       " 'accessible',\n",
       " 'spotting',\n",
       " 'strict',\n",
       " 'aliens',\n",
       " 'Commerce',\n",
       " 'Sudan',\n",
       " 'sustaining',\n",
       " 'Tom',\n",
       " 'Cambodia',\n",
       " 'Assistive',\n",
       " 'lured',\n",
       " 'Cameron',\n",
       " 'equations',\n",
       " 'Powergen',\n",
       " 'Camera',\n",
       " 'variation',\n",
       " 'Tags',\n",
       " 'Hong',\n",
       " '2015',\n",
       " 'troubles',\n",
       " 'hold',\n",
       " 'annoying',\n",
       " 'distinctive',\n",
       " 'reckons',\n",
       " 'Supt',\n",
       " 'prowess',\n",
       " 'abroad',\n",
       " 'sets',\n",
       " 'Nevis',\n",
       " 'wherever',\n",
       " 'inspiration',\n",
       " 'super',\n",
       " 'range',\n",
       " 'close',\n",
       " 'admit',\n",
       " 'urban',\n",
       " 'Stoddart',\n",
       " 'paper',\n",
       " 'Oregon',\n",
       " 'bookmarks',\n",
       " 'Panlogic',\n",
       " 'enhancements',\n",
       " 'Goldfinger',\n",
       " 'innovation',\n",
       " 'hinge',\n",
       " 'Toulouse',\n",
       " 'titled',\n",
       " 'P7131',\n",
       " 'rare',\n",
       " '66',\n",
       " 'addresses',\n",
       " 'territory',\n",
       " 'educationalists',\n",
       " 'dependable',\n",
       " 'deciding',\n",
       " 'elements',\n",
       " 'start',\n",
       " 'Civil',\n",
       " 'presidential',\n",
       " 'wadis',\n",
       " 'monitored',\n",
       " 'upbeat',\n",
       " 'restaurant',\n",
       " 'inadvertently',\n",
       " 'slower',\n",
       " 'flourished',\n",
       " 'Along',\n",
       " 'Developments',\n",
       " 'Hub',\n",
       " 'Ever',\n",
       " 'Japan',\n",
       " 'silly',\n",
       " 'partnership',\n",
       " 'external',\n",
       " 'Wanted',\n",
       " 'destination',\n",
       " 'ahead',\n",
       " 'Maine',\n",
       " 'applies',\n",
       " 'CBBC',\n",
       " 'reliant',\n",
       " 'slowly',\n",
       " 'Darkprofits',\n",
       " 'Datamonitor',\n",
       " 'hrs',\n",
       " 'Idol',\n",
       " 'ever',\n",
       " 'heats',\n",
       " 'invading',\n",
       " 'pancreatic',\n",
       " 'risen',\n",
       " 'chromium',\n",
       " 'dump',\n",
       " 'enthralling',\n",
       " 'favourably',\n",
       " 'emailed',\n",
       " 'correspondent',\n",
       " 'discipline',\n",
       " 'survives',\n",
       " '47',\n",
       " 'journals',\n",
       " 'GT4',\n",
       " 'textile',\n",
       " 'McAfee',\n",
       " 'passports',\n",
       " 'trailers',\n",
       " 'bleakest',\n",
       " 'defraud',\n",
       " 'reflection',\n",
       " 'iMovie',\n",
       " 'reference',\n",
       " 'Charles',\n",
       " 'Ericsson',\n",
       " 'Jowell',\n",
       " 'Websidestory',\n",
       " 'dry',\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_types = set(tech_tokens)\n",
    "print(\"types count : \" , len(tech_types))\n",
    "tech_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 9-d:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of repeat football in sports = 93\n",
      "count of repeat football in Technology = 8\n",
      "count of repeat computer in sports = 0\n",
      "count of repeat computer in Technology = 299\n"
     ]
    }
   ],
   "source": [
    "for word in ['football','computer']:\n",
    "    for text_name , tokens_list in [(\"sports\",sport_tokens) , (\"Technology\" ,tech_tokens) ]:\n",
    "        print(f\"count of repeat {word} in {text_name} = {tokens_list.count(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 9-e:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document = {}\n",
    "for word in sport_types.union(tech_types) :\n",
    "    term_document[word] = [sport_tokens.count(word),tech_tokens.count(word)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_matrix = pd.DataFrame(term_document.values() , index = term_document.keys() , columns=[\"sport\" ,\"tech\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>willingness</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognising</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Robotic</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Defenders</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firms</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partido</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supremacy</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debrief</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacuum</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19378 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sport  tech\n",
       "willingness      2     1\n",
       "computer         0   299\n",
       "recognising      0     1\n",
       "Robotic          0     1\n",
       "Defenders        1     1\n",
       "...            ...   ...\n",
       "firms            0   188\n",
       "Partido          1     0\n",
       "supremacy        0     3\n",
       "debrief          1     0\n",
       "vacuum           0     1\n",
       "\n",
       "[19378 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part 9-f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"football\",\"sport\",\"technology\",\"computer\",\"basketball\",\"laptop\",\"website\"]\n",
    "cos_similarities = numpy.zeros((7,7))\n",
    "for index1,word1 in enumerate(words):\n",
    "    for index2,word2 in enumerate(words):\n",
    "        cos_similarities[index1,index2]=1- cosine(term_doc_matrix.loc[word1].tolist() , term_doc_matrix.loc[word2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>football</th>\n",
       "      <th>sport</th>\n",
       "      <th>technology</th>\n",
       "      <th>computer</th>\n",
       "      <th>basketball</th>\n",
       "      <th>laptop</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>football</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988254</td>\n",
       "      <td>0.100045</td>\n",
       "      <td>0.085705</td>\n",
       "      <td>0.623970</td>\n",
       "      <td>0.085705</td>\n",
       "      <td>0.228332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.988254</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250923</td>\n",
       "      <td>0.236956</td>\n",
       "      <td>0.736062</td>\n",
       "      <td>0.236956</td>\n",
       "      <td>0.374434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>0.100045</td>\n",
       "      <td>0.250923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.839953</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.991542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer</th>\n",
       "      <td>0.085705</td>\n",
       "      <td>0.236956</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basketball</th>\n",
       "      <td>0.623970</td>\n",
       "      <td>0.736062</td>\n",
       "      <td>0.839953</td>\n",
       "      <td>0.832050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832050</td>\n",
       "      <td>0.903278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laptop</th>\n",
       "      <td>0.085705</td>\n",
       "      <td>0.236956</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>website</th>\n",
       "      <td>0.228332</td>\n",
       "      <td>0.374434</td>\n",
       "      <td>0.991542</td>\n",
       "      <td>0.989570</td>\n",
       "      <td>0.903278</td>\n",
       "      <td>0.989570</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            football     sport  technology  computer  basketball    laptop  \\\n",
       "football    1.000000  0.988254    0.100045  0.085705    0.623970  0.085705   \n",
       "sport       0.988254  1.000000    0.250923  0.236956    0.736062  0.236956   \n",
       "technology  0.100045  0.250923    1.000000  0.999896    0.839953  0.999896   \n",
       "computer    0.085705  0.236956    0.999896  1.000000    0.832050  1.000000   \n",
       "basketball  0.623970  0.736062    0.839953  0.832050    1.000000  0.832050   \n",
       "laptop      0.085705  0.236956    0.999896  1.000000    0.832050  1.000000   \n",
       "website     0.228332  0.374434    0.991542  0.989570    0.903278  0.989570   \n",
       "\n",
       "             website  \n",
       "football    0.228332  \n",
       "sport       0.374434  \n",
       "technology  0.991542  \n",
       "computer    0.989570  \n",
       "basketball  0.903278  \n",
       "laptop      0.989570  \n",
       "website     1.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cos_similarities , index=words , columns=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
