{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,precision_score,recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read csvs and make dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news_df = pandas.read_csv(\"News/True.csv\")\n",
    "fake_news_df = pandas.read_csv(\"News/Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news_df[\"data\"] = true_news_df[\"title\"] + true_news_df[\"text\"]\n",
    "fake_news_df[\"data\"] = fake_news_df[\"title\"] + fake_news_df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data\n",
    "remove single character words and stop_words and lemmatize each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lemmatize = nltk.stem.WordNetLemmatizer().lemmatize #function lemmatize\n",
    "def preprocess(data_list):\n",
    "    data_tokens_list = []\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "    for data in data_list:\n",
    "        data_tokens = \" \".join([lemmatize(token) for token in tokenizer.tokenize(data) if len(token)>1 and token.lower() not in stop_words])\n",
    "        data_tokens_list.append(data_tokens)\n",
    "        \n",
    "        \n",
    "    return data_tokens_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news_df = pandas.DataFrame({\n",
    "    \"content\":preprocess(true_news_df[\"data\"]),\n",
    "    \"lable\":1 # lable for True \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_df = pandas.DataFrame({\n",
    "    \"content\":preprocess(fake_news_df[\"data\"]),\n",
    "    \"lable\":0 # lable for fake \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pandas.concat([true_news_df , fake_news_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>budget fight loom Republicans flip fiscal scri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>military accept transgender recruit Monday Pen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Republican senator Let Mr Mueller job W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped Australian diplomat ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump want Postal Service charge much Amazon s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23476</th>\n",
       "      <td>McPain John McCain Furious Iran Treated US Sai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23477</th>\n",
       "      <td>JUSTICE Yahoo Settles mail Privacy Class actio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23478</th>\n",
       "      <td>Sunnistan US Allied Safe Zone Plan Take Territ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>Blow 700 Million Al Jazeera America Finally Ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23480</th>\n",
       "      <td>10 Navy Sailors Held Iranian Military Signs Ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  lable\n",
       "0      budget fight loom Republicans flip fiscal scri...      1\n",
       "1      military accept transgender recruit Monday Pen...      1\n",
       "2      Senior Republican senator Let Mr Mueller job W...      1\n",
       "3      FBI Russia probe helped Australian diplomat ti...      1\n",
       "4      Trump want Postal Service charge much Amazon s...      1\n",
       "...                                                  ...    ...\n",
       "23476  McPain John McCain Furious Iran Treated US Sai...      0\n",
       "23477  JUSTICE Yahoo Settles mail Privacy Class actio...      0\n",
       "23478  Sunnistan US Allied Safe Zone Plan Take Territ...      0\n",
       "23479  Blow 700 Million Al Jazeera America Finally Ca...      0\n",
       "23480  10 Navy Sailors Held Iranian Military Signs Ne...      0\n",
       "\n",
       "[44898 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split test and trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents ,test_contents ,train_lables,test_lables = train_test_split(news_df[\"content\"],news_df[\"lable\"],test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_of_classifier(train_data , train_lables , test_data , test_lables , classifier):\n",
    "    classifier.fit(train_data,train_lables)\n",
    "    predicts = classifier.predict(test_data)\n",
    "    conf_matrix = confusion_matrix(test_lables,predicts)\n",
    "    print(f\"accuracy = {accuracy_score(test_lables,predicts)}\\n\",    \n",
    "          f\"precision = {precision_score(test_lables,predicts)}\\n\",\n",
    "          f\"recall = {recall_score(test_lables,predicts)}\\n\",\n",
    "          f\"f1_score = {f1_score(test_lables,predicts)}\\n\",\n",
    "          f\"confusion_matrix = [{conf_matrix[0,0]}  {conf_matrix[0,1]}\\n\",\n",
    "          f\"                    {conf_matrix[1,0]}  {conf_matrix[1,1]}]\"\n",
    "         )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence to vectors and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "vectors = bag_of_words_vectorizer.fit_transform( train_contents.to_list()+test_contents.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectors[:len(train_contents)]\n",
    "test_vectors = vectors[len(train_contents):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with linear svm algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9789532293986637\n",
      " precision = 0.9827748938178386\n",
      " recall = 0.9729035272132679\n",
      " f1_score = 0.9778142974527526\n",
      " confusion_matrix = [4626  73\n",
      "                     116  4165]\n"
     ]
    }
   ],
   "source": [
    "report_of_classifier(train_vectors,train_lables,test_vectors,test_lables,SVC(kernel=\"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9790645879732739\n",
      " precision = 0.9684138246738384\n",
      " recall = 0.9883204858677879\n",
      " f1_score = 0.9782658959537572\n",
      " confusion_matrix = [4561  138\n",
      "                     50  4231]\n"
     ]
    }
   ],
   "source": [
    "report_of_classifier(train_vectors,train_lables,test_vectors,test_lables,MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "vectors = tf_idf_vectorizer.fit_transform( train_contents.to_list()+test_contents.to_list())\n",
    "train_vectors = vectors[:len(train_contents)]\n",
    "test_vectors = vectors[len(train_contents):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9944320712694877\n",
      " precision = 0.9923202234116826\n",
      " recall = 0.9960289651950479\n",
      " f1_score = 0.9941711354628118\n",
      " confusion_matrix = [4666  33\n",
      "                     17  4264]\n"
     ]
    }
   ],
   "source": [
    "report_of_classifier(train_vectors,train_lables,test_vectors,test_lables,SVC(kernel=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9400890868596882\n",
      " precision = 0.9465521355285135\n",
      " recall = 0.926652651249708\n",
      " f1_score = 0.9364966949952784\n",
      " confusion_matrix = [4475  224\n",
      "                     314  3967]\n"
     ]
    }
   ],
   "source": [
    "report_of_classifier(train_vectors,train_lables,test_vectors,test_lables,MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
